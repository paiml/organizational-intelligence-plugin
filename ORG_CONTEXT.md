# Project Context

**Language**: rust
**Project Path**: .

## Project Structure

- **Total Files**: 35
- **Total Functions**: 83
- **Median Cyclomatic**: 1.00
- **Median Cognitive**: 0.00

## Quality Scorecard

- **Overall Health**: 68.3%
- **Maintainability Index**: 70.0
- **Complexity Score**: 50.0
- **Test Coverage**: 65.0%

## Files

### ./.pmat-gates.toml


### ./.pre-commit-config.yaml


### ./benches/gpu_benchmarks.rs

**File Complexity**: 2 | **Functions**: 7

- **Function**: `bench_pearson_correlation` [complexity: 2] [cognitive: 1] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `bench_feature_extraction` [complexity: 3] [cognitive: 3] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `bench_storage_bulk_insert` [complexity: 2] [cognitive: 1] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `bench_storage_query` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `bench_to_vectors` [complexity: 2] [cognitive: 1] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `bench_gpu_vs_simd_correlation` [complexity: 3] [cognitive: 4] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `bench_speedup_validation` [complexity: 2] [cognitive: 3] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]

### ./demo.sh


### ./examples/analyze_org.rs

**File Complexity**: 8 | **Functions**: 1

- **Function**: `main` [complexity: 8] [cognitive: 9] [big-o: O(n log n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]

### ./examples/classify_defects.rs

**File Complexity**: 4 | **Functions**: 1

- **Function**: `main` [complexity: 4] [cognitive: 4] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]

### ./examples/full_analysis.rs

**File Complexity**: 12 | **Functions**: 1

- **Function**: `main` [complexity: 12] [cognitive: 18] [big-o: O(n log n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]

### ./pmat-quality.toml


### ./src/analyzer.rs

**File Complexity**: 1 | **Functions**: 0

- **Struct**: `OrgAnalyzer` [fields: 3]
- **Impl**: `impl OrgAnalyzer { # [doc = " Create a new organizational analyzer"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `cache_dir` - Directory for storing cloned repositories"] # [doc = ""] # [doc = " # Examples"] # [doc = " ```"] # [doc = " use organizational_intelligence_plugin::analyzer::OrgAnalyzer;"] # [doc = " use std::path::PathBuf;"] # [doc = ""] # [doc = " let analyzer = OrgAnalyzer::new(PathBuf::from(\"/tmp/repos\"));"] # [doc = " ```"] pub fn new < P : AsRef < Path > > (cache_dir : P) -> Self { let cache_dir = cache_dir . as_ref () . to_path_buf () ; Self { git_analyzer : GitAnalyzer :: new (& cache_dir) , classifier : RuleBasedClassifier :: new () , cache_dir , } } # [doc = " Analyze a single repository"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `repo_url` - Repository URL"] # [doc = " * `repo_name` - Repository name"] # [doc = " * `max_commits` - Maximum commits to analyze"] # [doc = ""] # [doc = " # Returns"] # [doc = " * `Ok(Vec<DefectPattern>)` with detected defect patterns"] # [doc = ""] # [doc = " # Examples"] # [doc = " ```no_run"] # [doc = " # use organizational_intelligence_plugin::analyzer::OrgAnalyzer;"] # [doc = " # use std::path::PathBuf;"] # [doc = " # async fn example() -> Result<(), anyhow::Error> {"] # [doc = " let analyzer = OrgAnalyzer::new(PathBuf::from(\"/tmp/repos\"));"] # [doc = " let patterns = analyzer.analyze_repository("] # [doc = "     \"https://github.com/rust-lang/rust\","] # [doc = "     \"rust\","] # [doc = "     1000"] # [doc = " ).await?;"] # [doc = " # Ok(())"] # [doc = " # }"] # [doc = " ```"] pub async fn analyze_repository (& self , repo_url : & str , repo_name : & str , max_commits : usize ,) -> Result < Vec < DefectPattern > > { info ! ("Analyzing repository {} (up to {} commits)" , repo_name , max_commits) ; self . git_analyzer . clone_repository (repo_url , repo_name) ? ; let commits = self . git_analyzer . analyze_commits (repo_name , max_commits) ? ; debug ! ("Retrieved {} commits from {}" , commits . len () , repo_name) ; let mut patterns = self . aggregate_defect_patterns (& commits) ; let repo_path = self . cache_dir . join (repo_name) ; if let Ok (tdg_analysis) = PmatIntegration :: analyze_tdg (& repo_path) { debug ! ("TDG analysis: avg={:.1}, max={:.1}" , tdg_analysis . average_score , tdg_analysis . max_score) ; self . enrich_with_tdg (& mut patterns , & tdg_analysis) ; } else { debug ! ("TDG analysis unavailable (pmat not installed or failed)") ; } info ! ("Found {} defect categories in {}" , patterns . len () , repo_name) ; Ok (patterns) } # [doc = " Aggregate defect patterns from classified commits"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `commits` - List of commits to analyze"] # [doc = ""] # [doc = " # Returns"] # [doc = " * `Vec<DefectPattern>` with aggregated statistics"] fn aggregate_defect_patterns (& self , commits : & [CommitInfo]) -> Vec < DefectPattern > { let mut category_map : HashMap < DefectCategory , CategoryStats > = HashMap :: new () ; for commit in commits { if let Some (classification) = self . classifier . classify_from_message (& commit . message) { let stats = category_map . entry (classification . category) . or_insert_with (| | CategoryStats :: new (classification . category)) ; stats . add_instance (commit , & classification) ; } } category_map . into_values () . map (| stats | stats . into_defect_pattern ()) . collect () } # [doc = " Enrich defect patterns with TDG quality signals"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `patterns` - Defect patterns to enrich"] # [doc = " * `tdg_analysis` - TDG analysis results"] fn enrich_with_tdg (& self , patterns : & mut [DefectPattern] , tdg_analysis : & TdgAnalysis) { for pattern in patterns . iter_mut () { pattern . quality_signals . avg_tdg_score = Some (tdg_analysis . average_score) ; pattern . quality_signals . max_tdg_score = Some (tdg_analysis . max_score) ; } } } . self_ty`
- **Struct**: `CategoryStats` [fields: 7]
- **Impl**: `impl CategoryStats { fn new (category : DefectCategory) -> Self { Self { category , count : 0 , total_confidence : 0.0 , instances : Vec :: new () , total_files_changed : 0 , total_lines_added : 0 , total_lines_removed : 0 , } } fn add_instance (& mut self , commit : & CommitInfo , classification : & Classification) { self . count += 1 ; self . total_confidence += classification . confidence ; self . total_files_changed += commit . files_changed ; self . total_lines_added += commit . lines_added ; self . total_lines_removed += commit . lines_removed ; if self . instances . len () < 3 { self . instances . push (DefectInstance { commit_hash : commit . hash [.. 8 . min (commit . hash . len ())] . to_string () , message : commit . message . clone () , author : commit . author . clone () , timestamp : commit . timestamp , files_affected : commit . files_changed , lines_added : commit . lines_added , lines_removed : commit . lines_removed , }) ; } } fn into_defect_pattern (self) -> DefectPattern { let avg_confidence = if self . count > 0 { self . total_confidence / self . count as f32 } else { 0.0 } ; let quality_signals = if self . count > 0 { QualitySignals { avg_tdg_score : None , max_tdg_score : None , avg_complexity : None , avg_test_coverage : None , satd_instances : 0 , avg_lines_changed : (self . total_lines_added + self . total_lines_removed) as f32 / self . count as f32 , avg_files_per_commit : self . total_files_changed as f32 / self . count as f32 , } } else { QualitySignals :: default () } ; DefectPattern { category : self . category , frequency : self . count , confidence : avg_confidence , quality_signals , examples : self . instances , } } } . self_ty`
- **Function**: `tests::test_org_analyzer_can_be_created` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_aggregate_empty_commits` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_aggregate_non_defect_commits` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_aggregate_defect_commits` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_category_stats_aggregation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_examples_limited_to_three` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_enrich_with_tdg` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_analyze_real_repository` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]

### ./src/classifier.rs

**File Complexity**: 1 | **Functions**: 0

- **Enum**: `DefectCategory` [variants: 10]
- **Impl**: `impl DefectCategory { # [doc = " Get human-readable name for the category"] pub fn as_str (& self) -> & 'static str { match self { Self :: MemorySafety => "Memory Safety" , Self :: ConcurrencyBugs => "Concurrency Bugs" , Self :: LogicErrors => "Logic Errors" , Self :: ApiMisuse => "API Misuse" , Self :: ResourceLeaks => "Resource Leaks" , Self :: TypeErrors => "Type Errors" , Self :: ConfigurationErrors => "Configuration Errors" , Self :: SecurityVulnerabilities => "Security Vulnerabilities" , Self :: PerformanceIssues => "Performance Issues" , Self :: IntegrationFailures => "Integration Failures" , } } } . self_ty`
- **Impl**: `fmt :: Display` for `impl fmt :: Display for DefectCategory { fn fmt (& self , f : & mut fmt :: Formatter < '_ >) -> fmt :: Result { match self { Self :: MemorySafety => write ! (f , "MemorySafety") , Self :: ConcurrencyBugs => write ! (f , "ConcurrencyBugs") , Self :: LogicErrors => write ! (f , "LogicErrors") , Self :: ApiMisuse => write ! (f , "ApiMisuse") , Self :: ResourceLeaks => write ! (f , "ResourceLeaks") , Self :: TypeErrors => write ! (f , "TypeErrors") , Self :: ConfigurationErrors => write ! (f , "ConfigurationErrors") , Self :: SecurityVulnerabilities => write ! (f , "SecurityVulnerabilities") , Self :: PerformanceIssues => write ! (f , "PerformanceIssues") , Self :: IntegrationFailures => write ! (f , "IntegrationFailures") , } } } . self_ty`
- **Struct**: `Classification` [fields: 4]
- **Struct**: `Rule` [fields: 3]
- **Struct**: `RuleBasedClassifier` [fields: 1]
- **Impl**: `impl RuleBasedClassifier { # [doc = " Create a new rule-based classifier with predefined patterns"] # [doc = ""] # [doc = " # Examples"] # [doc = " ```"] # [doc = " use organizational_intelligence_plugin::classifier::RuleBasedClassifier;"] # [doc = ""] # [doc = " let classifier = RuleBasedClassifier::new();"] # [doc = " ```"] pub fn new () -> Self { let rules = vec ! [Rule { category : DefectCategory :: MemorySafety , patterns : vec ! ["use after free" , "use-after-free" , "null pointer" , "nullptr" , "buffer overflow" , "memory leak" , "dangling pointer" , "double free" , "heap corruption" ,] , confidence : 0.85 , } , Rule { category : DefectCategory :: ConcurrencyBugs , patterns : vec ! ["race condition" , "data race" , "deadlock" , "atomicity" , "thread safety" , "concurrent" , "synchronization" , "mutex" , "lock contention" ,] , confidence : 0.80 , } , Rule { category : DefectCategory :: SecurityVulnerabilities , patterns : vec ! ["sql injection" , "xss" , "cross-site scripting" , "authentication" , "authorization" , "security" , "vulnerability" , "exploit" , "cve-" ,] , confidence : 0.90 , } , Rule { category : DefectCategory :: LogicErrors , patterns : vec ! ["off by one" , "off-by-one" , "boundary" , "incorrect logic" , "wrong condition" , "infinite loop" ,] , confidence : 0.70 , } , Rule { category : DefectCategory :: ApiMisuse , patterns : vec ! ["api misuse" , "wrong parameter" , "incorrect usage" , "missing error handling" , "unchecked error" ,] , confidence : 0.75 , } , Rule { category : DefectCategory :: ResourceLeaks , patterns : vec ! ["resource leak" , "file handle leak" , "connection leak" , "not closed" , "forgot to close" ,] , confidence : 0.80 , } , Rule { category : DefectCategory :: TypeErrors , patterns : vec ! ["type error" , "type mismatch" , "casting error" , "serialization" , "deserialization" ,] , confidence : 0.75 , } , Rule { category : DefectCategory :: ConfigurationErrors , patterns : vec ! ["configuration" , "config" , "environment variable" , "missing env" , "settings" ,] , confidence : 0.70 , } , Rule { category : DefectCategory :: PerformanceIssues , patterns : vec ! ["performance" , "slow" , "inefficient" , "n+1 query" , "optimization" ,] , confidence : 0.65 , } , Rule { category : DefectCategory :: IntegrationFailures , patterns : vec ! ["integration" , "compatibility" , "version mismatch" , "breaking change" , "api change" ,] , confidence : 0.70 , } ,] ; Self { rules } } # [doc = " Classify a defect based on commit message"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `message` - Commit message text"] # [doc = ""] # [doc = " # Returns"] # [doc = " * `Some(Classification)` if patterns match"] # [doc = " * `None` if no patterns match (not a defect fix)"] # [doc = ""] # [doc = " # Examples"] # [doc = " ```"] # [doc = " use organizational_intelligence_plugin::classifier::RuleBasedClassifier;"] # [doc = ""] # [doc = " let classifier = RuleBasedClassifier::new();"] # [doc = " let result = classifier.classify_from_message(\"fix: null pointer dereference\");"] # [doc = ""] # [doc = " assert!(result.is_some());"] # [doc = " ```"] pub fn classify_from_message (& self , message : & str) -> Option < Classification > { let message_lower = message . to_lowercase () ; debug ! ("Classifying message: {}" , message) ; let mut matches : Vec < (DefectCategory , f32 , Vec < String >) > = Vec :: new () ; for rule in & self . rules { let mut matched_patterns = Vec :: new () ; for pattern in & rule . patterns { if message_lower . contains (pattern) { matched_patterns . push (pattern . to_string ()) ; } } if ! matched_patterns . is_empty () { let confidence_boost = (matched_patterns . len () - 1) as f32 * 0.05 ; let adjusted_confidence = (rule . confidence + confidence_boost) . min (0.95) ; matches . push ((rule . category , adjusted_confidence , matched_patterns)) ; } } if matches . is_empty () { debug ! ("No patterns matched for message") ; return None ; } matches . sort_by (| a , b | b . 1 . partial_cmp (& a . 1) . unwrap ()) ; let (category , confidence , matched_patterns) = matches . into_iter () . next () . unwrap () ; let explanation = format ! ("Classified as '{}' based on patterns: {}. Confidence: {:.0}%" , category . as_str () , matched_patterns . join (", ") , confidence * 100.0) ; debug ! ("Classification: {:?} with confidence {}" , category , confidence) ; Some (Classification { category , confidence , explanation , matched_patterns , }) } } . self_ty`
- **Impl**: `Default` for `impl Default for RuleBasedClassifier { fn default () -> Self { Self :: new () } } . self_ty`
- **Function**: `tests::test_classifier_creation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_all_categories_covered` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_pattern_matching` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_non_defect_returns_none` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]

### ./src/cli.rs

**File Complexity**: 1 | **Functions**: 0

- **Struct**: `Cli` [fields: 3]
- **Enum**: `Commands` [variants: 3]
- **Function**: `tests::test_cli_structure_exists` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]

### ./src/config.rs

**File Complexity**: 1 | **Functions**: 0

- **Struct**: `Config` [fields: 5]
- **Struct**: `AnalysisConfig` [fields: 4]
- **Impl**: `Default` for `impl Default for AnalysisConfig { fn default () -> Self { Self { max_commits : 1000 , workers : num_cpus :: get () . max (1) , cache_dir : ".oip-cache" . to_string () , include_merges : false , } } } . self_ty`
- **Struct**: `MlConfig` [fields: 6]
- **Impl**: `Default` for `impl Default for MlConfig { fn default () -> Self { Self { n_trees : 100 , max_depth : 10 , k_clusters : 5 , max_iterations : 100 , smote_k : 5 , smote_ratio : 0.5 , } } } . self_ty`
- **Struct**: `StorageConfig` [fields: 3]
- **Impl**: `Default` for `impl Default for StorageConfig { fn default () -> Self { Self { default_output : "oip-gpu.db" . to_string () , compress : true , batch_size : 1000 , } } } . self_ty`
- **Struct**: `ComputeConfig` [fields: 3]
- **Impl**: `Default` for `impl Default for ComputeConfig { fn default () -> Self { Self { backend : "auto" . to_string () , workgroup_size : 256 , gpu_enabled : true , } } } . self_ty`
- **Struct**: `LoggingConfig` [fields: 3]
- **Impl**: `Default` for `impl Default for LoggingConfig { fn default () -> Self { Self { level : "info" . to_string () , json : false , file : None , } } } . self_ty`
- **Impl**: `impl Config { # [doc = " Load configuration from file"] pub fn from_file (path : & Path) -> Result < Self > { let content = std :: fs :: read_to_string (path) ? ; let config : Config = serde_yaml :: from_str (& content) ? ; Ok (config) } # [doc = " Load configuration with environment overrides"] pub fn load (path : Option < & Path >) -> Result < Self > { let mut config = if let Some (p) = path { if p . exists () { Self :: from_file (p) ? } else { Self :: default () } } else { let default_paths = [".oip.yaml" , ".oip.yml" , "oip.yaml" , "oip.yml"] ; let mut found = None ; for p in & default_paths { let path = Path :: new (p) ; if path . exists () { found = Some (Self :: from_file (path) ?) ; break ; } } found . unwrap_or_default () } ; config . apply_env_overrides () ; Ok (config) } # [doc = " Apply environment variable overrides"] fn apply_env_overrides (& mut self) { if let Ok (val) = std :: env :: var ("OIP_MAX_COMMITS") { if let Ok (n) = val . parse () { self . analysis . max_commits = n ; } } if let Ok (val) = std :: env :: var ("OIP_WORKERS") { if let Ok (n) = val . parse () { self . analysis . workers = n ; } } if let Ok (val) = std :: env :: var ("OIP_CACHE_DIR") { self . analysis . cache_dir = val ; } if let Ok (val) = std :: env :: var ("OIP_K_CLUSTERS") { if let Ok (n) = val . parse () { self . ml . k_clusters = n ; } } if let Ok (val) = std :: env :: var ("OIP_BACKEND") { self . compute . backend = val ; } if let Ok (val) = std :: env :: var ("OIP_GPU_ENABLED") { self . compute . gpu_enabled = val == "1" || val . to_lowercase () == "true" ; } if let Ok (val) = std :: env :: var ("OIP_LOG_LEVEL") { self . logging . level = val ; } if let Ok (val) = std :: env :: var ("OIP_LOG_JSON") { self . logging . json = val == "1" || val . to_lowercase () == "true" ; } } # [doc = " Save configuration to file"] pub fn save (& self , path : & Path) -> Result < () > { let content = serde_yaml :: to_string (self) ? ; std :: fs :: write (path , content) ? ; Ok (()) } # [doc = " Validate configuration"] pub fn validate (& self) -> Result < () > { if self . analysis . max_commits == 0 { anyhow :: bail ! ("max_commits must be > 0") ; } if self . analysis . workers == 0 { anyhow :: bail ! ("workers must be > 0") ; } if self . ml . k_clusters == 0 { anyhow :: bail ! ("k_clusters must be > 0") ; } if self . ml . smote_ratio <= 0.0 || self . ml . smote_ratio > 1.0 { anyhow :: bail ! ("smote_ratio must be in (0, 1]") ; } Ok (()) } # [doc = " Generate example configuration"] pub fn example_yaml () -> String { let config = Config :: default () ; serde_yaml :: to_string (& config) . unwrap_or_default () } } . self_ty`
- **Function**: `tests::test_default_config` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_config_validation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_invalid_config` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_config_save_load` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_example_yaml` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_load_missing_file` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]

### ./src/correlation.rs

**File Complexity**: 7 | **Functions**: 1

- **Struct**: `CorrelationMatrix` [fields: 2]
- **Function**: `pearson_correlation` [complexity: 7] [cognitive: 8] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Struct**: `CachedCorrelation` [fields: 4]
- **Impl**: `impl CachedCorrelation { # [doc = " Create cached correlation with default capacity (1000 entries, 5 min TTL)"] pub fn new () -> Self { Self :: with_capacity (1000) } # [doc = " Create cached correlation with custom capacity"] pub fn with_capacity (capacity : usize) -> Self { Self { cache : LruCache :: with_ttl (capacity , Duration :: from_secs (300)) , stats : PerfStats :: new () , cache_hits : 0 , cache_misses : 0 , } } # [doc = " Compute correlation with caching"] # [doc = ""] # [doc = " Cache key is based on vector indices (assumes stable vector ordering)"] pub fn correlate (& mut self , x : & Vector < f32 > , y : & Vector < f32 > , x_idx : usize , y_idx : usize ,) -> Result < f32 > { let key = if x_idx <= y_idx { (x_idx , y_idx) } else { (y_idx , x_idx) } ; if let Some (cached) = self . cache . get (& key) { self . cache_hits += 1 ; return Ok (cached) ; } self . cache_misses += 1 ; let start = Instant :: now () ; let result = pearson_correlation (x , y) ? ; let duration_ns = start . elapsed () . as_nanos () as u64 ; self . stats . record (duration_ns) ; self . cache . insert (key , result) ; Ok (result) } # [doc = " Compute correlation matrix for multiple vectors"] pub fn correlation_matrix (& mut self , vectors : & [Vector < f32 >]) -> Result < Vec < Vec < f32 > > > { let n = vectors . len () ; let mut matrix = vec ! [vec ! [0.0 ; n] ; n] ; for i in 0 .. n { matrix [i] [i] = 1.0 ; for j in (i + 1) .. n { let corr = self . correlate (& vectors [i] , & vectors [j] , i , j) ? ; matrix [i] [j] = corr ; matrix [j] [i] = corr ; } } Ok (matrix) } # [doc = " Get performance statistics"] pub fn stats (& self) -> & PerfStats { & self . stats } # [doc = " Get cache hit rate"] pub fn cache_hit_rate (& self) -> f64 { let total = self . cache_hits + self . cache_misses ; if total == 0 { 0.0 } else { self . cache_hits as f64 / total as f64 } } # [doc = " Get cache statistics"] pub fn cache_stats (& self) -> (u64 , u64) { (self . cache_hits , self . cache_misses) } # [doc = " Clear cache"] pub fn clear_cache (& mut self) { self . cache . clear () ; self . cache_hits = 0 ; self . cache_misses = 0 ; } } . self_ty`
- **Impl**: `Default` for `impl Default for CachedCorrelation { fn default () -> Self { Self :: new () } } . self_ty`
- **Function**: `tests::test_pearson_perfect_positive` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_pearson_perfect_negative` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_pearson_zero_variance` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_pearson_different_lengths` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_cached_correlation_creation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_cached_correlation_compute` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_cached_correlation_cache_hit` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_cached_correlation_symmetric_key` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_cached_correlation_matrix` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_cached_correlation_clear` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]

### ./src/error.rs

**File Complexity**: 1 | **Functions**: 0

- **Enum**: `OipError` [variants: 18]
- **Impl**: `impl OipError { pub fn no_data (context : impl Into < String >) -> Self { Self :: NoData { context : context . into () , } } pub fn invalid_data (message : impl Into < String >) -> Self { Self :: InvalidData { message : message . into () , } } pub fn validation (field : impl Into < String > , reason : impl Into < String >) -> Self { Self :: ValidationError { field : field . into () , reason : reason . into () , } } pub fn github (message : impl Into < String >) -> Self { Self :: GitHubError { message : message . into () , } } pub fn repo_not_found (repo : impl Into < String >) -> Self { Self :: RepoNotFound { repo : repo . into () } } pub fn git (operation : impl Into < String > , reason : impl Into < String >) -> Self { Self :: GitError { operation : operation . into () , reason : reason . into () , } } pub fn auth_required (message : impl Into < String >) -> Self { Self :: AuthRequired { message : message . into () , } } pub fn insufficient_data (operation : impl Into < String > , required : usize , actual : usize) -> Self { Self :: InsufficientData { operation : operation . into () , required , actual , } } pub fn compute (operation : impl Into < String > , reason : impl Into < String >) -> Self { Self :: ComputeError { operation : operation . into () , reason : reason . into () , } } pub fn gpu_unavailable (reason : impl Into < String >) -> Self { Self :: GpuUnavailable { reason : reason . into () , } } pub fn storage (operation : impl Into < String > , reason : impl Into < String >) -> Self { Self :: StorageError { operation : operation . into () , reason : reason . into () , } } pub fn file_not_found (path : impl Into < String >) -> Self { Self :: FileNotFound { path : path . into () } } pub fn io (context : impl Into < String > , source : std :: io :: Error) -> Self { Self :: IoError { context : context . into () , source , } } pub fn config (message : impl Into < String >) -> Self { Self :: ConfigError { message : message . into () , } } pub fn invalid_arg (arg : impl Into < String > , reason : impl Into < String >) -> Self { Self :: InvalidArgument { arg : arg . into () , reason : reason . into () , } } pub fn failed (message : impl Into < String >) -> Self { Self :: OperationFailed { message : message . into () , } } # [doc = " Get a user-friendly recovery hint for this error"] pub fn recovery_hint (& self) -> Option < & 'static str > { match self { Self :: NoData { .. } => Some ("Try analyzing a repository first with 'oip-gpu analyze'") , Self :: RepoNotFound { .. } => { Some ("Check the repository name format (owner/repo) and ensure it exists") } Self :: AuthRequired { .. } => Some ("Set GITHUB_TOKEN environment variable") , Self :: ModelNotTrained => Some ("Train the model first with predictor.train(features)") , Self :: InsufficientData { .. } => Some ("Provide more training data or reduce k value") , Self :: GpuUnavailable { .. } => { Some ("Use --backend simd for CPU fallback, or install GPU drivers") } Self :: FileNotFound { .. } => Some ("Check the file path and ensure it exists") , Self :: ConfigError { .. } => Some ("Check configuration file syntax (YAML/TOML)") , Self :: InvalidArgument { .. } => Some ("Run with --help to see valid arguments") , _ => None , } } # [doc = " Check if this error is recoverable"] pub fn is_recoverable (& self) -> bool { matches ! (self , Self :: NoData { .. } | Self :: RepoNotFound { .. } | Self :: AuthRequired { .. } | Self :: ModelNotTrained | Self :: InsufficientData { .. } | Self :: GpuUnavailable { .. } | Self :: FileNotFound { .. } | Self :: ConfigError { .. } | Self :: InvalidArgument { .. }) } # [doc = " Get error category for logging/metrics"] pub fn category (& self) -> & 'static str { match self { Self :: NoData { .. } | Self :: InvalidData { .. } | Self :: ValidationError { .. } => "data" , Self :: GitHubError { .. } | Self :: RepoNotFound { .. } | Self :: GitError { .. } | Self :: AuthRequired { .. } => "git" , Self :: ModelNotTrained | Self :: InsufficientData { .. } | Self :: ComputeError { .. } | Self :: GpuUnavailable { .. } => "compute" , Self :: StorageError { .. } | Self :: FileNotFound { .. } | Self :: IoError { .. } => { "storage" } Self :: ConfigError { .. } | Self :: InvalidArgument { .. } => "config" , Self :: OperationFailed { .. } | Self :: Other (_) => "other" , } } } . self_ty`
- **Trait**: `ResultExt`
- **Impl**: `ResultExt < T >` for `impl < T , E : Into < OipError > > ResultExt < T > for Result < T , E > { fn context (self , context : impl Into < String >) -> OipResult < T > { self . map_err (| e | { let inner = e . into () ; OipError :: OperationFailed { message : format ! ("{}: {}" , context . into () , inner) , } }) } fn with_context < F , S > (self , f : F) -> OipResult < T > where F : FnOnce () -> S , S : Into < String > , { self . map_err (| e | { let inner = e . into () ; OipError :: OperationFailed { message : format ! ("{}: {}" , f () . into () , inner) , } }) } } . self_ty`
- **Function**: `tests::test_error_display` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_error_recovery_hint` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_error_is_recoverable` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_error_category` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_insufficient_data_error` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_validation_error` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_result_context` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]

### ./src/features.rs

**File Complexity**: 1 | **Functions**: 0

- **Struct**: `CommitFeatures` [fields: 8]
- **Impl**: `impl CommitFeatures { # [doc = " Convert to flat vector for GPU processing"] # [doc = ""] # [doc = " Fixed-size vector enables efficient GPU batching"] pub fn to_vector (& self) -> Vec < f32 > { vec ! [self . defect_category as f32 , self . files_changed , self . lines_added , self . lines_deleted , self . complexity_delta , self . timestamp as f32 , self . hour_of_day as f32 , self . day_of_week as f32 ,] } # [doc = " Vector dimension count (for GPU buffer allocation)"] pub const DIMENSION : usize = 8 ; } . self_ty`
- **Struct**: `FeatureExtractor` [fields: 0]
- **Impl**: `impl FeatureExtractor { pub fn new () -> Self { Self } # [doc = " Extract features from defect category and metadata"] pub fn extract (& self , category : u8 , files_changed : usize , lines_added : usize , lines_deleted : usize , timestamp : i64 ,) -> Result < CommitFeatures > { let datetime = chrono :: DateTime :: from_timestamp (timestamp , 0) . ok_or_else (| | anyhow :: anyhow ! ("Invalid timestamp")) ? ; let hour_of_day = datetime . hour () as u8 ; let day_of_week = datetime . weekday () . num_days_from_monday () as u8 ; Ok (CommitFeatures { defect_category : category , files_changed : files_changed as f32 , lines_added : lines_added as f32 , lines_deleted : lines_deleted as f32 , complexity_delta : 0.0 , timestamp : timestamp as f64 , hour_of_day , day_of_week , }) } } . self_ty`
- **Impl**: `Default` for `impl Default for FeatureExtractor { fn default () -> Self { Self :: new () } } . self_ty`
- **Struct**: `FeatureInput` [fields: 5]
- **Struct**: `BatchFeatureExtractor` [fields: 3]
- **Impl**: `impl BatchFeatureExtractor { # [doc = " Create batch extractor with default batch size (1000)"] pub fn new () -> Self { Self :: with_batch_size (1000) } # [doc = " Create batch extractor with custom batch size"] pub fn with_batch_size (batch_size : usize) -> Self { Self { extractor : FeatureExtractor :: new () , batch_processor : BatchProcessor :: new (batch_size) , stats : PerfStats :: new () , } } # [doc = " Add input to batch, returns extracted features if batch is full"] pub fn add (& mut self , input : FeatureInput) -> Option < Vec < CommitFeatures > > { self . batch_processor . add (input) . map (| batch | self . extract_batch (batch)) } # [doc = " Flush remaining inputs and extract features"] pub fn flush (& mut self) -> Vec < CommitFeatures > { let batch = self . batch_processor . flush () ; if batch . is_empty () { Vec :: new () } else { self . extract_batch (batch) } } # [doc = " Extract features from batch with performance tracking"] fn extract_batch (& mut self , inputs : Vec < FeatureInput >) -> Vec < CommitFeatures > { let start = Instant :: now () ; let features : Vec < CommitFeatures > = inputs . into_iter () . filter_map (| input | { self . extractor . extract (input . category , input . files_changed , input . lines_added , input . lines_deleted , input . timestamp ,) . ok () }) . collect () ; let duration_ns = start . elapsed () . as_nanos () as u64 ; self . stats . record (duration_ns) ; features } # [doc = " Extract all features at once (convenience method)"] pub fn extract_all (& mut self , inputs : Vec < FeatureInput >) -> Vec < CommitFeatures > { let start = Instant :: now () ; let features : Vec < CommitFeatures > = inputs . into_iter () . filter_map (| input | { self . extractor . extract (input . category , input . files_changed , input . lines_added , input . lines_deleted , input . timestamp ,) . ok () }) . collect () ; let duration_ns = start . elapsed () . as_nanos () as u64 ; self . stats . record (duration_ns) ; features } # [doc = " Get performance statistics"] pub fn stats (& self) -> & PerfStats { & self . stats } # [doc = " Get pending item count"] pub fn pending (& self) -> usize { self . batch_processor . len () } } . self_ty`
- **Impl**: `Default` for `impl Default for BatchFeatureExtractor { fn default () -> Self { Self :: new () } } . self_ty`
- **Function**: `tests::test_feature_extractor_creation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_extract_basic_features` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_to_vector_dimension` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_temporal_features` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_invalid_timestamp` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_batch_extractor_creation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_batch_extractor_add` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_batch_extractor_flush` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_batch_extractor_extract_all` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_batch_extractor_stats` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]

### ./src/git.rs

**File Complexity**: 1 | **Functions**: 0

- **Struct**: `CommitInfo` [fields: 7]
- **Struct**: `GitAnalyzer` [fields: 1]
- **Impl**: `impl GitAnalyzer { # [doc = " Create a new GitAnalyzer with specified cache directory"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `cache_dir` - Directory to store cloned repositories"] # [doc = ""] # [doc = " # Examples"] # [doc = " ```"] # [doc = " use organizational_intelligence_plugin::git::GitAnalyzer;"] # [doc = " use std::path::PathBuf;"] # [doc = ""] # [doc = " let analyzer = GitAnalyzer::new(PathBuf::from(\"/tmp/repos\"));"] # [doc = " ```"] pub fn new < P : AsRef < Path > > (cache_dir : P) -> Self { let cache_dir = cache_dir . as_ref () . to_path_buf () ; Self { cache_dir } } # [doc = " Clone a repository to the cache directory"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `repo_url` - Git repository URL (https)"] # [doc = " * `name` - Local name for the repository"] # [doc = ""] # [doc = " # Returns"] # [doc = " * `Ok(())` if successful"] # [doc = " * `Err` if clone fails"] # [doc = ""] # [doc = " # Examples"] # [doc = " ```no_run"] # [doc = " # use organizational_intelligence_plugin::git::GitAnalyzer;"] # [doc = " # use std::path::PathBuf;"] # [doc = " # async fn example() -> Result<(), anyhow::Error> {"] # [doc = " let analyzer = GitAnalyzer::new(PathBuf::from(\"/tmp/repos\"));"] # [doc = " analyzer.clone_repository(\"https://github.com/rust-lang/rust\", \"rust\")?;"] # [doc = " # Ok(())"] # [doc = " # }"] # [doc = " ```"] pub fn clone_repository (& self , repo_url : & str , name : & str) -> Result < () > { let repo_path = self . cache_dir . join (name) ; if repo_path . exists () { debug ! ("Repository {} already exists at {:?}" , name , repo_path) ; return Ok (()) ; } info ! ("Cloning repository {} from {}" , name , repo_url) ; Repository :: clone (repo_url , & repo_path) . map_err (| e | { anyhow ! ("Failed to clone repository {} from {}: {}" , name , repo_url , e) }) ? ; info ! ("Successfully cloned {} to {:?}" , name , repo_path) ; Ok (()) } # [doc = " Analyze commits in a cloned repository"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `name` - Repository name (must be already cloned)"] # [doc = " * `limit` - Maximum number of commits to analyze"] # [doc = ""] # [doc = " # Returns"] # [doc = " * `Ok(Vec<CommitInfo>)` with commit information"] # [doc = " * `Err` if repository not found or analysis fails"] # [doc = ""] # [doc = " # Examples"] # [doc = " ```no_run"] # [doc = " # use organizational_intelligence_plugin::git::GitAnalyzer;"] # [doc = " # use std::path::PathBuf;"] # [doc = " # async fn example() -> Result<(), anyhow::Error> {"] # [doc = " let analyzer = GitAnalyzer::new(PathBuf::from(\"/tmp/repos\"));"] # [doc = " analyzer.clone_repository(\"https://github.com/rust-lang/rust\", \"rust\")?;"] # [doc = " let commits = analyzer.analyze_commits(\"rust\", 100)?;"] # [doc = " # Ok(())"] # [doc = " # }"] # [doc = " ```"] pub fn analyze_commits (& self , name : & str , limit : usize) -> Result < Vec < CommitInfo > > { let repo_path = self . cache_dir . join (name) ; if ! repo_path . exists () { return Err (anyhow ! ("Repository {} not found at {:?}. Clone it first." , name , repo_path)) ; } debug ! ("Opening repository at {:?}" , repo_path) ; let repo = Repository :: open (& repo_path) . map_err (| e | anyhow ! ("Failed to open repository {}: {}" , name , e)) ? ; let mut revwalk = repo . revwalk () ? ; revwalk . push_head () ? ; let mut commits = Vec :: new () ; for (i , oid) in revwalk . enumerate () { if i >= limit { break ; } let oid = oid ? ; let commit = repo . find_commit (oid) ? ; let hash = commit . id () . to_string () ; let message = commit . message () . unwrap_or ("") . to_string () ; let author = commit . author () . email () . unwrap_or ("unknown") . to_string () ; let timestamp = commit . time () . seconds () ; let (files_changed , lines_added , lines_removed) = if commit . parent_count () > 0 { let parent = commit . parent (0) ? ; let diff = repo . diff_tree_to_tree (Some (& parent . tree () ?) , Some (& commit . tree () ?) , None) ? ; let stats = diff . stats () ? ; (stats . files_changed () , stats . insertions () , stats . deletions ()) } else { let tree = commit . tree () ? ; (tree . len () , 0 , 0) } ; commits . push (CommitInfo { hash , message , author , timestamp , files_changed , lines_added , lines_removed , }) ; } debug ! ("Analyzed {} commits from {}" , commits . len () , name) ; Ok (commits) } } . self_ty`
- **Function**: `tests::test_git_analyzer_can_be_created` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_commit_info_structure` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_analyze_nonexistent_repo` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_clone_small_repository` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_analyze_commits_basic` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_analyze_commits_respects_limit` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_analyzer_caches_cloned_repos` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]

### ./src/github.rs

**File Complexity**: 1 | **Functions**: 0

- **Struct**: `RepoInfo` [fields: 7]
- **Struct**: `GitHubMiner` [fields: 1]
- **Impl**: `impl GitHubMiner { # [doc = " Create a new GitHub miner"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `token` - Optional GitHub personal access token for authenticated requests"] # [doc = ""] # [doc = " # Examples"] # [doc = " ```no_run"] # [doc = " use organizational_intelligence_plugin::github::GitHubMiner;"] # [doc = ""] # [doc = " // Public repos only (unauthenticated)"] # [doc = " let miner = GitHubMiner::new(None);"] # [doc = ""] # [doc = " // With authentication (higher rate limits)"] # [doc = " let miner_auth = GitHubMiner::new(Some(\"ghp_token\".to_string()));"] # [doc = " ```"] pub fn new (token : Option < String >) -> Self { let client = if let Some (token) = token { debug ! ("Initializing GitHub client with authentication") ; Octocrab :: builder () . personal_token (token) . build () . expect ("Failed to build Octocrab client") } else { debug ! ("Initializing GitHub client without authentication") ; Octocrab :: builder () . build () . expect ("Failed to build Octocrab client") } ; Self { client } } # [doc = " Fetch all repositories for an organization"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `org_name` - GitHub organization name"] # [doc = ""] # [doc = " # Errors"] # [doc = " Returns error if:"] # [doc = " - Organization name is empty"] # [doc = " - API request fails"] # [doc = " - Organization doesn't exist"] # [doc = ""] # [doc = " # Examples"] # [doc = " ```no_run"] # [doc = " use organizational_intelligence_plugin::github::GitHubMiner;"] # [doc = ""] # [doc = " # async fn example() -> Result<(), anyhow::Error> {"] # [doc = " let miner = GitHubMiner::new(None);"] # [doc = " let repos = miner.fetch_organization_repos(\"rust-lang\").await?;"] # [doc = " println!(\"Found {} repositories\", repos.len());"] # [doc = " # Ok(())"] # [doc = " # }"] # [doc = " ```"] pub async fn fetch_organization_repos (& self , org_name : & str) -> Result < Vec < RepoInfo > > { if org_name . trim () . is_empty () { return Err (anyhow ! ("Organization name cannot be empty")) ; } info ! ("Fetching repositories for organization: {}" , org_name) ; let repos = self . client . orgs (org_name) . list_repos () . send () . await . map_err (| e | anyhow ! ("Failed to fetch repositories for {}: {}" , org_name , e)) ? ; debug ! ("Found {} repositories for {}" , repos . items . len () , org_name) ; let repo_infos : Vec < RepoInfo > = repos . items . into_iter () . map (| repo | RepoInfo { name : repo . name , full_name : repo . full_name . unwrap_or_default () , description : repo . description , language : repo . language . and_then (| v | v . as_str () . map (String :: from)) , stars : repo . stargazers_count . unwrap_or (0) , default_branch : repo . default_branch . unwrap_or_else (| | "main" . to_string ()) , updated_at : repo . updated_at . unwrap_or_else (Utc :: now) , }) . collect () ; info ! ("Successfully fetched {} repositories for {}" , repo_infos . len () , org_name) ; Ok (repo_infos) } # [doc = " Filter repositories by last update date"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `repos` - List of repositories to filter"] # [doc = " * `since` - Only include repos updated since this date"] # [doc = ""] # [doc = " # Returns"] # [doc = " Filtered list of repositories"] pub fn filter_by_date (repos : Vec < RepoInfo > , since : DateTime < Utc >) -> Vec < RepoInfo > { repos . into_iter () . filter (| repo | repo . updated_at >= since) . collect () } } . self_ty`
- **Function**: `tests::test_github_miner_creation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_empty_org_name_validation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_whitespace_org_name_validation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]

### ./src/gpu_correlation.rs

**File Complexity**: 1 | **Functions**: 0

- **Struct**: `GpuCorrelationEngine` [fields: 4]
- **Impl**: `# [cfg (feature = "gpu")] impl GpuCorrelationEngine { # [doc = " Initialize GPU correlation engine"] # [doc = ""] # [doc = " Requires GPU hardware with Vulkan/Metal/DX12 support"] pub async fn new () -> Result < Self > { let instance = wgpu :: Instance :: new (wgpu :: InstanceDescriptor { backends : wgpu :: Backends :: all () , .. Default :: default () }) ; let adapter = instance . request_adapter (& wgpu :: RequestAdapterOptions { power_preference : wgpu :: PowerPreference :: HighPerformance , force_fallback_adapter : false , compatible_surface : None , }) . await . ok_or_else (| | anyhow :: anyhow ! ("Failed to find GPU adapter")) ? ; let (device , queue) = adapter . request_device (& wgpu :: DeviceDescriptor { label : Some ("GPU Correlation Device") , required_features : wgpu :: Features :: empty () , required_limits : wgpu :: Limits :: default () , } , None ,) . await ? ; let bind_group_layout = device . create_bind_group_layout (& wgpu :: BindGroupLayoutDescriptor { label : Some ("Correlation Bind Group Layout") , entries : & [wgpu :: BindGroupLayoutEntry { binding : 0 , visibility : wgpu :: ShaderStages :: COMPUTE , ty : wgpu :: BindingType :: Buffer { ty : wgpu :: BufferBindingType :: Storage { read_only : true } , has_dynamic_offset : false , min_binding_size : None , } , count : None , } , wgpu :: BindGroupLayoutEntry { binding : 1 , visibility : wgpu :: ShaderStages :: COMPUTE , ty : wgpu :: BindingType :: Buffer { ty : wgpu :: BufferBindingType :: Storage { read_only : true } , has_dynamic_offset : false , min_binding_size : None , } , count : None , } , wgpu :: BindGroupLayoutEntry { binding : 2 , visibility : wgpu :: ShaderStages :: COMPUTE , ty : wgpu :: BindingType :: Buffer { ty : wgpu :: BufferBindingType :: Storage { read_only : false } , has_dynamic_offset : false , min_binding_size : None , } , count : None , } , wgpu :: BindGroupLayoutEntry { binding : 3 , visibility : wgpu :: ShaderStages :: COMPUTE , ty : wgpu :: BindingType :: Buffer { ty : wgpu :: BufferBindingType :: Uniform , has_dynamic_offset : false , min_binding_size : None , } , count : None , } ,] , }) ; let shader = device . create_shader_module (wgpu :: ShaderModuleDescriptor { label : Some ("Correlation Shader") , source : wgpu :: ShaderSource :: Wgsl (CORRELATION_SHADER . into ()) , }) ; let pipeline_layout = device . create_pipeline_layout (& wgpu :: PipelineLayoutDescriptor { label : Some ("Correlation Pipeline Layout") , bind_group_layouts : & [& bind_group_layout] , push_constant_ranges : & [] , }) ; let pipeline = device . create_compute_pipeline (& wgpu :: ComputePipelineDescriptor { label : Some ("Correlation Pipeline") , layout : Some (& pipeline_layout) , module : & shader , entry_point : "correlation_kernel" , }) ; Ok (Self { device , queue , pipeline , bind_group_layout , }) } # [doc = " Compute correlation between two vectors on GPU"] # [doc = ""] # [doc = " Returns correlation coefficient in range [-1, 1]"] pub async fn correlate (& self , data_a : & [f32] , data_b : & [f32]) -> Result < f32 > { if data_a . len () != data_b . len () { anyhow :: bail ! ("Vectors must have same length") ; } let size = data_a . len () as u32 ; let buffer_a = self . device . create_buffer_init (& wgpu :: util :: BufferInitDescriptor { label : Some ("Data A") , contents : bytemuck :: cast_slice (data_a) , usage : wgpu :: BufferUsages :: STORAGE | wgpu :: BufferUsages :: COPY_DST , }) ; let buffer_b = self . device . create_buffer_init (& wgpu :: util :: BufferInitDescriptor { label : Some ("Data B") , contents : bytemuck :: cast_slice (data_b) , usage : wgpu :: BufferUsages :: STORAGE | wgpu :: BufferUsages :: COPY_DST , }) ; let result_buffer = self . device . create_buffer (& wgpu :: BufferDescriptor { label : Some ("Result") , size : std :: mem :: size_of :: < f32 > () as u64 , usage : wgpu :: BufferUsages :: STORAGE | wgpu :: BufferUsages :: COPY_SRC , mapped_at_creation : false , }) ; let size_buffer = self . device . create_buffer_init (& wgpu :: util :: BufferInitDescriptor { label : Some ("Size Uniform") , contents : bytemuck :: cast_slice (& [size]) , usage : wgpu :: BufferUsages :: UNIFORM , }) ; let staging_buffer = self . device . create_buffer (& wgpu :: BufferDescriptor { label : Some ("Staging Buffer") , size : std :: mem :: size_of :: < f32 > () as u64 , usage : wgpu :: BufferUsages :: MAP_READ | wgpu :: BufferUsages :: COPY_DST , mapped_at_creation : false , }) ; let bind_group = self . device . create_bind_group (& wgpu :: BindGroupDescriptor { label : Some ("Correlation Bind Group") , layout : & self . bind_group_layout , entries : & [wgpu :: BindGroupEntry { binding : 0 , resource : buffer_a . as_entire_binding () , } , wgpu :: BindGroupEntry { binding : 1 , resource : buffer_b . as_entire_binding () , } , wgpu :: BindGroupEntry { binding : 2 , resource : result_buffer . as_entire_binding () , } , wgpu :: BindGroupEntry { binding : 3 , resource : size_buffer . as_entire_binding () , } ,] , }) ; let mut encoder = self . device . create_command_encoder (& wgpu :: CommandEncoderDescriptor { label : Some ("Correlation Encoder") , }) ; { let mut compute_pass = encoder . begin_compute_pass (& wgpu :: ComputePassDescriptor { label : Some ("Correlation Pass") , timestamp_writes : None , }) ; compute_pass . set_pipeline (& self . pipeline) ; compute_pass . set_bind_group (0 , & bind_group , & []) ; compute_pass . dispatch_workgroups (1 , 1 , 1) ; } encoder . copy_buffer_to_buffer (& result_buffer , 0 , & staging_buffer , 0 , std :: mem :: size_of :: < f32 > () as u64 ,) ; self . queue . submit (Some (encoder . finish ())) ; let buffer_slice = staging_buffer . slice (..) ; let (sender , receiver) = futures :: channel :: oneshot :: channel () ; buffer_slice . map_async (wgpu :: MapMode :: Read , move | result | { sender . send (result) . ok () ; }) ; self . device . poll (wgpu :: Maintain :: Wait) ; receiver . await ? ? ; let data = buffer_slice . get_mapped_range () ; let result : f32 = bytemuck :: cast_slice (& data) [0] ; drop (data) ; staging_buffer . unmap () ; Ok (result) } } . self_ty`
- **Struct**: `GpuCorrelationEngine` [fields: 0]
- **Impl**: `# [cfg (not (feature = "gpu"))] impl GpuCorrelationEngine { pub async fn new () -> Result < Self , String > { Err ("GPU feature not enabled. Compile with --features gpu" . to_string ()) } } . self_ty`
- **Function**: `tests::assert_gpu_cpu_equivalent` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_gpu_engine_creation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_gpu_correlation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_gpu_cpu_equivalence_perfect_positive` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_gpu_cpu_equivalence_perfect_negative` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_gpu_cpu_equivalence_zero_correlation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_gpu_cpu_equivalence_large_vectors` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_gpu_cpu_equivalence_random_data` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]

### ./src/gpu_main.rs

**File Complexity**: 6 | **Functions**: 14

- **Struct**: `Cli` [fields: 4]
- **Enum**: `Backend` [variants: 3]
- **Enum**: `Commands` [variants: 8]
- **Enum**: `OutputFormat` [variants: 4]
- **Enum**: `ExportFormat` [variants: 4]
- **Enum**: `GraphAlgorithm` [variants: 3]
- **Enum**: `BenchmarkSuite` [variants: 4]
- **Function**: `main` [complexity: 12] [cognitive: 19] [big-o: O(n log n)] [provability: 43%] [satd: 0] [churn: low(5)] [tdg: 2.5]
- **Function**: `cmd_analyze` [complexity: 15] [cognitive: 21] [big-o: O(n log n)] [provability: 43%] [satd: 0] [churn: low(5)] [tdg: 2.5]
- **Function**: `cmd_analyze_local` [complexity: 22] [cognitive: 39] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(5)] [tdg: 2.5]
- **Function**: `cmd_correlate` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(5)] [tdg: 2.5]
- **Function**: `cmd_predict` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(5)] [tdg: 2.5]
- **Function**: `cmd_query` [complexity: 10] [cognitive: 14] [big-o: O(n log n)] [provability: 43%] [satd: 0] [churn: low(5)] [tdg: 2.5]
- **Function**: `execute_query` [complexity: 3] [cognitive: 3] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(5)] [tdg: 2.5]
- **Function**: `count_by_category` [complexity: 3] [cognitive: 3] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(5)] [tdg: 2.5]
- **Enum**: `QueryResult` [variants: 2]
- **Function**: `print_table` [complexity: 6] [cognitive: 11] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(5)] [tdg: 2.5]
- **Function**: `print_csv` [complexity: 6] [cognitive: 11] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(5)] [tdg: 2.5]
- **Function**: `cmd_cluster` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(5)] [tdg: 2.5]
- **Function**: `cmd_graph` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(5)] [tdg: 2.5]
- **Function**: `cmd_export` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(5)] [tdg: 2.5]
- **Function**: `cmd_benchmark` [complexity: 7] [cognitive: 6] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(5)] [tdg: 2.5]

### ./src/gpu_store.rs

**File Complexity**: 1 | **Functions**: 0

- **Struct**: `GPUHotStore` [fields: 2]
- **Impl**: `impl GPUHotStore { # [doc = " Create new hot store"] pub fn new () -> Result < Self > { Ok (Self { feature_count : 0 , commit_count : 0 , }) } # [doc = " Get feature dimensions"] pub fn dimensions (& self) -> (usize , usize) { (self . commit_count , self . feature_count) } } . self_ty`
- **Impl**: `Default` for `impl Default for GPUHotStore { fn default () -> Self { Self :: new () . unwrap () } } . self_ty`
- **Function**: `tests::test_hot_store_creation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_dimensions_initial` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]

### ./src/imbalance.rs

**File Complexity**: 1 | **Functions**: 0

- **Struct**: `Smote` [fields: 1]
- **Impl**: `impl Smote { # [doc = " Create SMOTE with default k=5 neighbors"] pub fn new () -> Self { Self { k_neighbors : 5 } } # [doc = " Create SMOTE with custom k neighbors"] pub fn with_k (k_neighbors : usize) -> Self { Self { k_neighbors } } # [doc = " Oversample minority class to balance dataset"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `features` - All features (majority + minority)"] # [doc = " * `minority_category` - Category ID to oversample (0-9)"] # [doc = " * `target_ratio` - Target minority:majority ratio (e.g., 0.5 = 50%)"] # [doc = ""] # [doc = " # Returns"] # [doc = " Original features + synthetic minority samples"] pub fn oversample (& self , features : & [CommitFeatures] , minority_category : u8 , target_ratio : f32 ,) -> Result < Vec < CommitFeatures > > { let minority : Vec < & CommitFeatures > = features . iter () . filter (| f | f . defect_category == minority_category) . collect () ; let majority_count = features . len () - minority . len () ; if minority . is_empty () { anyhow :: bail ! ("No samples found for category {}" , minority_category) ; } let target_minority = (majority_count as f32 * target_ratio) as usize ; let samples_needed = target_minority . saturating_sub (minority . len ()) ; if samples_needed == 0 { return Ok (features . to_vec ()) ; } let minority_vecs : Vec < Vec < f32 > > = minority . iter () . map (| f | f . to_vector ()) . collect () ; let mut synthetic = Vec :: with_capacity (samples_needed) ; let mut sample_idx = 0 ; while synthetic . len () < samples_needed { let base_idx = sample_idx % minority . len () ; let base = & minority_vecs [base_idx] ; let neighbors = self . find_k_nearest (base , & minority_vecs , base_idx) ; let neighbor_idx = neighbors [sample_idx % neighbors . len ()] ; let neighbor = & minority_vecs [neighbor_idx] ; let synthetic_vec = self . interpolate (base , neighbor , sample_idx) ; let synthetic_feature = self . vector_to_features (& synthetic_vec , minority_category) ; synthetic . push (synthetic_feature) ; sample_idx += 1 ; } let mut result = features . to_vec () ; result . extend (synthetic) ; Ok (result) } # [doc = " Find k nearest neighbors using Euclidean distance"] fn find_k_nearest (& self , target : & [f32] , all : & [Vec < f32 >] , exclude_idx : usize) -> Vec < usize > { let mut distances : Vec < (usize , f32) > = all . iter () . enumerate () . filter (| (i , _) | * i != exclude_idx) . map (| (i , v) | (i , self . euclidean_distance (target , v))) . collect () ; distances . sort_by (| a , b | a . 1 . partial_cmp (& b . 1) . unwrap ()) ; distances . iter () . take (self . k_neighbors . min (distances . len ())) . map (| (i , _) | * i) . collect () } # [doc = " Euclidean distance between two vectors"] fn euclidean_distance (& self , a : & [f32] , b : & [f32]) -> f32 { a . iter () . zip (b . iter ()) . map (| (x , y) | (x - y) . powi (2)) . sum :: < f32 > () . sqrt () } # [doc = " Interpolate between two vectors (SMOTE core algorithm)"] fn interpolate (& self , base : & [f32] , neighbor : & [f32] , seed : usize) -> Vec < f32 > { let gap = ((seed * 17 + 42) % 100) as f32 / 100.0 ; base . iter () . zip (neighbor . iter ()) . map (| (b , n) | b + gap * (n - b)) . collect () } # [doc = " Convert vector back to CommitFeatures"] fn vector_to_features (& self , vec : & [f32] , category : u8) -> CommitFeatures { CommitFeatures { defect_category : category , files_changed : vec [1] . max (0.0) , lines_added : vec [2] . max (0.0) , lines_deleted : vec [3] . max (0.0) , complexity_delta : vec [4] , timestamp : vec [5] as f64 , hour_of_day : (vec [6] as u8) . min (23) , day_of_week : (vec [7] as u8) . min (6) , } } } . self_ty`
- **Impl**: `Default` for `impl Default for Smote { fn default () -> Self { Self :: new () } } . self_ty`
- **Struct**: `FocalLoss` [fields: 2]
- **Impl**: `impl FocalLoss { # [doc = " Create Focal Loss with default parameters"] pub fn new () -> Self { Self { gamma : 2.0 , alpha : vec ! [1.0 ; 10] , } } # [doc = " Create Focal Loss with custom gamma and class weights"] pub fn with_params (gamma : f32 , alpha : Vec < f32 >) -> Self { Self { gamma , alpha } } # [doc = " Compute class weights from sample distribution"] # [doc = ""] # [doc = " Inverse frequency weighting: _i = N / (K * n_i)"] # [doc = " Where N = total samples, K = classes, n_i = samples in class i"] pub fn compute_weights (features : & [CommitFeatures]) -> Vec < f32 > { let mut counts = [0usize ; 10] ; for f in features { let idx = (f . defect_category as usize) . min (9) ; counts [idx] += 1 ; } let total = features . len () as f32 ; let k = counts . iter () . filter (| & & c | c > 0) . count () as f32 ; counts . iter () . map (| & c | if c > 0 { total / (k * c as f32) } else { 0.0 }) . collect () } # [doc = " Compute focal loss for a single prediction"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `prob` - Predicted probability for correct class (0-1)"] # [doc = " * `class` - True class label (0-9)"] pub fn loss (& self , prob : f32 , class : u8) -> f32 { let p_t = prob . clamp (1e-7 , 1.0 - 1e-7) ; let alpha_t = self . alpha . get (class as usize) . copied () . unwrap_or (1.0) ; - alpha_t * (1.0 - p_t) . powf (self . gamma) * p_t . ln () } # [doc = " Compute batch focal loss"] pub fn batch_loss (& self , probs : & [f32] , classes : & [u8]) -> f32 { probs . iter () . zip (classes . iter ()) . map (| (& p , & c) | self . loss (p , c)) . sum :: < f32 > () / probs . len () as f32 } } . self_ty`
- **Impl**: `Default` for `impl Default for FocalLoss { fn default () -> Self { Self :: new () } } . self_ty`
- **Struct**: `AuprcMetric` [fields: 0]
- **Impl**: `impl AuprcMetric { # [doc = " Compute AUPRC from predictions and labels"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `predictions` - Predicted probabilities (0-1)"] # [doc = " * `labels` - True binary labels (0 or 1)"] # [doc = ""] # [doc = " # Returns"] # [doc = " AUPRC score (0-1, higher is better)"] pub fn compute (predictions : & [f32] , labels : & [u8]) -> Result < f32 > { if predictions . len () != labels . len () { anyhow :: bail ! ("Predictions and labels must have same length") ; } if predictions . is_empty () { anyhow :: bail ! ("Empty predictions") ; } let mut pairs : Vec < (f32 , u8) > = predictions . iter () . copied () . zip (labels . iter () . copied ()) . collect () ; pairs . sort_by (| a , b | b . 0 . partial_cmp (& a . 0) . unwrap ()) ; let total_positives = labels . iter () . filter (| & & l | l == 1) . count () as f32 ; if total_positives == 0.0 { anyhow :: bail ! ("No positive samples in labels") ; } let mut true_positives = 0.0 ; let mut false_positives = 0.0 ; let mut auprc = 0.0 ; let mut prev_recall = 0.0 ; for (_ , label) in & pairs { if * label == 1 { true_positives += 1.0 ; } else { false_positives += 1.0 ; } let precision = true_positives / (true_positives + false_positives) ; let recall = true_positives / total_positives ; auprc += precision * (recall - prev_recall) ; prev_recall = recall ; } Ok (auprc) } # [doc = " Compute precision at given recall threshold"] pub fn precision_at_recall (predictions : & [f32] , labels : & [u8] , target_recall : f32 ,) -> Result < f32 > { if predictions . len () != labels . len () { anyhow :: bail ! ("Predictions and labels must have same length") ; } let mut pairs : Vec < (f32 , u8) > = predictions . iter () . copied () . zip (labels . iter () . copied ()) . collect () ; pairs . sort_by (| a , b | b . 0 . partial_cmp (& a . 0) . unwrap ()) ; let total_positives = labels . iter () . filter (| & & l | l == 1) . count () as f32 ; if total_positives == 0.0 { anyhow :: bail ! ("No positive samples") ; } let mut true_positives = 0.0 ; let mut false_positives = 0.0 ; for (_ , label) in & pairs { if * label == 1 { true_positives += 1.0 ; } else { false_positives += 1.0 ; } let recall = true_positives / total_positives ; if recall >= target_recall { let precision = true_positives / (true_positives + false_positives) ; return Ok (precision) ; } } Ok (0.0) } } . self_ty`
- **Function**: `tests::make_feature` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_smote_creation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_smote_oversample` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_smote_no_minority` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_focal_loss_creation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_focal_loss_compute_weights` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_focal_loss_value` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_auprc_perfect` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_auprc_range` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_precision_at_recall` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]

### ./src/lib.rs

**File Complexity**: 1 | **Functions**: 0


### ./src/main.rs

**File Complexity**: 19 | **Functions**: 1

- **Function**: `main` [complexity: 19] [cognitive: 44] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: med(8)] [tdg: 2.5]

### ./src/ml.rs

**File Complexity**: 1 | **Functions**: 0

- **Struct**: `DefectPredictor` [fields: 4]
- **Impl**: `impl DefectPredictor { # [doc = " Create new predictor with default parameters"] pub fn new () -> Self { Self { n_trees : 100 , max_depth : 10 , trained : false , training_data : Vec :: new () , } } # [doc = " Create predictor with custom parameters"] pub fn with_params (n_trees : usize , max_depth : usize) -> Self { Self { n_trees , max_depth , trained : false , training_data : Vec :: new () , } } # [doc = " Train model on labeled features"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `features` - Training features with defect_category labels"] pub fn train (& mut self , features : & [CommitFeatures]) -> Result < () > { if features . is_empty () { anyhow :: bail ! ("Cannot train on empty dataset") ; } self . training_data = features . iter () . map (| f | (f . to_vector () , f . defect_category)) . collect () ; self . trained = true ; Ok (()) } # [doc = " Predict defect category for new features"] # [doc = ""] # [doc = " Uses k-NN approximation (k=5) for Phase 1"] # [doc = " Full Random Forest in Phase 2 with aprender"] pub fn predict (& self , features : & CommitFeatures) -> Result < u8 > { if ! self . trained { anyhow :: bail ! ("Model not trained") ; } let query = features . to_vector () ; let k = 5 . min (self . training_data . len ()) ; let mut distances : Vec < (f32 , u8) > = self . training_data . iter () . map (| (v , label) | (Self :: euclidean_distance (& query , v) , * label)) . collect () ; distances . sort_by (| a , b | a . 0 . partial_cmp (& b . 0) . unwrap ()) ; let mut votes = [0u32 ; 10] ; for (_ , label) in distances . iter () . take (k) { let idx = (* label as usize) . min (9) ; votes [idx] += 1 ; } let prediction = votes . iter () . enumerate () . max_by_key (| (_ , & count) | count) . map (| (idx , _) | idx as u8) . unwrap_or (0) ; Ok (prediction) } # [doc = " Predict probabilities for all categories"] pub fn predict_proba (& self , features : & CommitFeatures) -> Result < Vec < f32 > > { if ! self . trained { anyhow :: bail ! ("Model not trained") ; } let query = features . to_vector () ; let k = 10 . min (self . training_data . len ()) ; let mut distances : Vec < (f32 , u8) > = self . training_data . iter () . map (| (v , label) | (Self :: euclidean_distance (& query , v) , * label)) . collect () ; distances . sort_by (| a , b | a . 0 . partial_cmp (& b . 0) . unwrap ()) ; let mut counts = [0u32 ; 10] ; for (_ , label) in distances . iter () . take (k) { let idx = (* label as usize) . min (9) ; counts [idx] += 1 ; } let probs : Vec < f32 > = counts . iter () . map (| & c | c as f32 / k as f32) . collect () ; Ok (probs) } # [doc = " Get model parameters"] pub fn params (& self) -> (usize , usize) { (self . n_trees , self . max_depth) } # [doc = " Check if model is trained"] pub fn is_trained (& self) -> bool { self . trained } fn euclidean_distance (a : & [f32] , b : & [f32]) -> f32 { a . iter () . zip (b . iter ()) . map (| (x , y) | (x - y) . powi (2)) . sum :: < f32 > () . sqrt () } } . self_ty`
- **Impl**: `Default` for `impl Default for DefectPredictor { fn default () -> Self { Self :: new () } } . self_ty`
- **Struct**: `PatternClusterer` [fields: 4]
- **Impl**: `impl PatternClusterer { # [doc = " Create clusterer with default k=5 clusters"] pub fn new () -> Self { Self { k : 5 , max_iterations : 100 , centroids : Vec :: new () , trained : false , } } # [doc = " Create clusterer with custom k"] pub fn with_k (k : usize) -> Self { Self { k , max_iterations : 100 , centroids : Vec :: new () , trained : false , } } # [doc = " Fit clusters to data"] pub fn fit (& mut self , features : & [CommitFeatures]) -> Result < () > { if features . is_empty () { anyhow :: bail ! ("Cannot cluster empty dataset") ; } if features . len () < self . k { anyhow :: bail ! ("Need at least {} samples for {} clusters" , self . k , self . k) ; } let vectors : Vec < Vec < f32 > > = features . iter () . map (| f | f . to_vector ()) . collect () ; let n_dims = CommitFeatures :: DIMENSION ; self . centroids = vectors . iter () . take (self . k) . cloned () . collect () ; for _ in 0 .. self . max_iterations { let assignments : Vec < usize > = vectors . iter () . map (| v | self . nearest_centroid (v)) . collect () ; let mut new_centroids = vec ! [vec ! [0.0 ; n_dims] ; self . k] ; let mut counts = vec ! [0usize ; self . k] ; for (vec , & cluster) in vectors . iter () . zip (assignments . iter ()) { for (dim , & val) in vec . iter () . enumerate () { new_centroids [cluster] [dim] += val ; } counts [cluster] += 1 ; } for (centroid , & count) in new_centroids . iter_mut () . zip (counts . iter ()) { if count > 0 { for val in centroid . iter_mut () { * val /= count as f32 ; } } } let converged = self . centroids . iter () . zip (new_centroids . iter ()) . all (| (old , new) | Self :: euclidean_distance (old , new) < 1e-6) ; self . centroids = new_centroids ; if converged { break ; } } self . trained = true ; Ok (()) } # [doc = " Predict cluster for new features"] pub fn predict (& self , features : & CommitFeatures) -> Result < usize > { if ! self . trained { anyhow :: bail ! ("Clusterer not fitted") ; } let vec = features . to_vector () ; Ok (self . nearest_centroid (& vec)) } # [doc = " Predict clusters for multiple features"] pub fn predict_batch (& self , features : & [CommitFeatures]) -> Result < Vec < usize > > { if ! self . trained { anyhow :: bail ! ("Clusterer not fitted") ; } Ok (features . iter () . map (| f | self . nearest_centroid (& f . to_vector ())) . collect ()) } # [doc = " Get cluster centroids"] pub fn centroids (& self) -> & [Vec < f32 >] { & self . centroids } # [doc = " Compute inertia (sum of squared distances to centroids)"] pub fn inertia (& self , features : & [CommitFeatures]) -> Result < f32 > { if ! self . trained { anyhow :: bail ! ("Clusterer not fitted") ; } let total : f32 = features . iter () . map (| f | { let vec = f . to_vector () ; let cluster = self . nearest_centroid (& vec) ; Self :: euclidean_distance (& vec , & self . centroids [cluster]) . powi (2) }) . sum () ; Ok (total) } fn nearest_centroid (& self , point : & [f32]) -> usize { self . centroids . iter () . enumerate () . map (| (i , c) | (i , Self :: euclidean_distance (point , c))) . min_by (| a , b | a . 1 . partial_cmp (& b . 1) . unwrap ()) . map (| (i , _) | i) . unwrap_or (0) } fn euclidean_distance (a : & [f32] , b : & [f32]) -> f32 { a . iter () . zip (b . iter ()) . map (| (x , y) | (x - y) . powi (2)) . sum :: < f32 > () . sqrt () } } . self_ty`
- **Impl**: `Default` for `impl Default for PatternClusterer { fn default () -> Self { Self :: new () } } . self_ty`
- **Struct**: `ModelMetrics` [fields: 0]
- **Impl**: `impl ModelMetrics { # [doc = " Compute accuracy"] pub fn accuracy (predictions : & [u8] , labels : & [u8]) -> f32 { if predictions . len () != labels . len () || predictions . is_empty () { return 0.0 ; } let correct = predictions . iter () . zip (labels . iter ()) . filter (| (p , l) | p == l) . count () ; correct as f32 / predictions . len () as f32 } # [doc = " Compute per-class precision"] pub fn precision (predictions : & [u8] , labels : & [u8] , class : u8) -> f32 { let true_positives = predictions . iter () . zip (labels . iter ()) . filter (| (& p , & l) | p == class && l == class) . count () as f32 ; let predicted_positives = predictions . iter () . filter (| & & p | p == class) . count () as f32 ; if predicted_positives > 0.0 { true_positives / predicted_positives } else { 0.0 } } # [doc = " Compute per-class recall"] pub fn recall (predictions : & [u8] , labels : & [u8] , class : u8) -> f32 { let true_positives = predictions . iter () . zip (labels . iter ()) . filter (| (& p , & l) | p == class && l == class) . count () as f32 ; let actual_positives = labels . iter () . filter (| & & l | l == class) . count () as f32 ; if actual_positives > 0.0 { true_positives / actual_positives } else { 0.0 } } # [doc = " Compute F1 score"] pub fn f1_score (predictions : & [u8] , labels : & [u8] , class : u8) -> f32 { let p = Self :: precision (predictions , labels , class) ; let r = Self :: recall (predictions , labels , class) ; if p + r > 0.0 { 2.0 * p * r / (p + r) } else { 0.0 } } # [doc = " Compute silhouette score for clustering"] pub fn silhouette_score (features : & [CommitFeatures] , assignments : & [usize] , k : usize) -> f32 { if features . len () != assignments . len () || features . is_empty () { return 0.0 ; } let vectors : Vec < Vec < f32 > > = features . iter () . map (| f | f . to_vector ()) . collect () ; let mut total_score = 0.0 ; let n = vectors . len () ; for i in 0 .. n { let cluster_i = assignments [i] ; let same_cluster : Vec < _ > = (0 .. n) . filter (| & j | j != i && assignments [j] == cluster_i) . collect () ; let a = if same_cluster . is_empty () { 0.0 } else { same_cluster . iter () . map (| & j | Self :: euclidean_distance (& vectors [i] , & vectors [j])) . sum :: < f32 > () / same_cluster . len () as f32 } ; let mut b = f32 :: INFINITY ; for c in 0 .. k { if c == cluster_i { continue ; } let other_cluster : Vec < _ > = (0 .. n) . filter (| & j | assignments [j] == c) . collect () ; if ! other_cluster . is_empty () { let mean_dist = other_cluster . iter () . map (| & j | Self :: euclidean_distance (& vectors [i] , & vectors [j])) . sum :: < f32 > () / other_cluster . len () as f32 ; b = b . min (mean_dist) ; } } if b . is_infinite () { b = 0.0 ; } let s = if a . max (b) > 0.0 { (b - a) / a . max (b) } else { 0.0 } ; total_score += s ; } total_score / n as f32 } fn euclidean_distance (a : & [f32] , b : & [f32]) -> f32 { a . iter () . zip (b . iter ()) . map (| (x , y) | (x - y) . powi (2)) . sum :: < f32 > () . sqrt () } } . self_ty`
- **Function**: `tests::make_feature` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_predictor_creation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_predictor_train` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_predictor_predict` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_predictor_proba` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_clusterer_creation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_clusterer_fit` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_clusterer_predict` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_clusterer_inertia` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_metrics_accuracy` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_metrics_precision_recall` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_metrics_f1` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_silhouette_score` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]

### ./src/observability.rs

**File Complexity**: 1 | **Functions**: 2

- **Function**: `init_tracing` [complexity: 2] [cognitive: 1] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `init_with_filter` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Struct**: `AnalysisSpan` [fields: 2]
- **Impl**: `impl AnalysisSpan { pub fn new (repo : impl Into < String > , operation : impl Into < String >) -> Self { Self { repo : repo . into () , operation : operation . into () , } } } . self_ty`
- **Struct**: `LogOps` [fields: 0]
- **Impl**: `impl LogOps { # [doc = " Log start of analysis"] # [instrument (skip_all , fields (repo = % repo , commits = commits))] pub fn analysis_start (repo : & str , commits : usize) { info ! ("Starting repository analysis") ; } # [doc = " Log analysis completion"] # [instrument (skip_all , fields (repo = % repo , patterns = patterns , duration_ms = duration_ms))] pub fn analysis_complete (repo : & str , patterns : usize , duration_ms : u64) { info ! ("Analysis complete") ; } # [doc = " Log feature extraction"] # [instrument (skip_all , fields (count = count))] pub fn features_extracted (count : usize) { debug ! ("Features extracted") ; } # [doc = " Log correlation computation"] # [instrument (skip_all , fields (size = size , backend = % backend))] pub fn correlation_start (size : usize , backend : & str) { debug ! ("Computing correlation") ; } # [doc = " Log correlation result"] # [instrument (skip_all , fields (result = % format ! ("{:.4}" , result) , duration_us = duration_us))] pub fn correlation_complete (result : f32 , duration_us : u64) { trace ! ("Correlation computed") ; } # [doc = " Log ML training"] # [instrument (skip_all , fields (model = % model , samples = samples))] pub fn training_start (model : & str , samples : usize) { info ! ("Training model") ; } # [doc = " Log ML training complete"] # [instrument (skip_all , fields (model = % model , accuracy = % format ! ("{:.2}%" , accuracy * 100.0)))] pub fn training_complete (model : & str , accuracy : f32) { info ! ("Model training complete") ; } # [doc = " Log prediction"] # [instrument (skip_all , fields (model = % model))] pub fn prediction (model : & str , category : u8) { debug ! (category = category , "Prediction made") ; } # [doc = " Log clustering"] # [instrument (skip_all , fields (k = k , samples = samples))] pub fn clustering_start (k : usize , samples : usize) { debug ! ("Starting clustering") ; } # [doc = " Log clustering complete"] # [instrument (skip_all , fields (k = k , inertia = % format ! ("{:.2}" , inertia)))] pub fn clustering_complete (k : usize , inertia : f32) { debug ! ("Clustering complete") ; } # [doc = " Log storage operation"] # [instrument (skip_all , fields (operation = % operation , count = count))] pub fn storage_op (operation : & str , count : usize) { trace ! ("Storage operation") ; } # [doc = " Log GPU operation"] # [instrument (skip_all , fields (operation = % operation , backend = % backend))] pub fn gpu_op (operation : & str , backend : & str) { debug ! ("GPU operation") ; } # [doc = " Log error with context"] pub fn error_with_context (error : & impl std :: fmt :: Display , context : & str) { error ! (context = context , error = % error , "Operation failed") ; } # [doc = " Log warning"] pub fn warning (message : & str , context : & str) { warn ! (context = context , "{}" , message) ; } # [doc = " Log performance metric"] # [instrument (skip_all , fields (operation = % operation , duration_ms = duration_ms , throughput = throughput))] pub fn performance (operation : & str , duration_ms : u64 , throughput : Option < f64 >) { if let Some (tp) = throughput { info ! (throughput_per_sec = % format ! ("{:.2}" , tp) , "Performance metric") ; } else { info ! ("Performance metric") ; } } } . self_ty`
- **Struct**: `Timer` [fields: 2]
- **Impl**: `impl Timer { pub fn new (operation : impl Into < String >) -> Self { Self { start : std :: time :: Instant :: now () , operation : operation . into () , } } pub fn elapsed_ms (& self) -> u64 { self . start . elapsed () . as_millis () as u64 } pub fn elapsed_us (& self) -> u64 { self . start . elapsed () . as_micros () as u64 } pub fn log_completion (& self) { let duration = self . elapsed_ms () ; debug ! (operation = % self . operation , duration_ms = duration , "Operation completed") ; } } . self_ty`
- **Impl**: `Drop` for `impl Drop for Timer { fn drop (& mut self) { } } . self_ty`
- **Struct**: `Metrics` [fields: 6]
- **Impl**: `impl Metrics { pub fn new () -> Self { Self :: default () } pub fn record_analysis (& mut self , duration_ms : u64) { self . analyses_count += 1 ; self . total_duration_ms += duration_ms ; } pub fn record_features (& mut self , count : u64) { self . features_extracted += count ; } pub fn record_correlation (& mut self) { self . correlations_computed += 1 ; } pub fn record_prediction (& mut self) { self . predictions_made += 1 ; } pub fn record_error (& mut self) { self . errors_count += 1 ; } pub fn summary (& self) -> String { format ! ("Metrics: analyses={}, features={}, correlations={}, predictions={}, errors={}, total_time={}ms" , self . analyses_count , self . features_extracted , self . correlations_computed , self . predictions_made , self . errors_count , self . total_duration_ms) } pub fn log_summary (& self) { info ! (analyses = self . analyses_count , features = self . features_extracted , correlations = self . correlations_computed , predictions = self . predictions_made , errors = self . errors_count , total_duration_ms = self . total_duration_ms , "Session metrics") ; } } . self_ty`
- **Function**: `tests::test_timer_creation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_timer_elapsed` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_metrics_default` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_metrics_recording` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_metrics_summary` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_analysis_span` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]

### ./src/perf.rs

**File Complexity**: 2 | **Functions**: 3

- **Struct**: `BatchProcessor` [fields: 2]
- **Impl**: `impl < T > BatchProcessor < T > { # [doc = " Create new batch processor"] pub fn new (batch_size : usize) -> Self { Self { batch_size , buffer : Vec :: with_capacity (batch_size) , } } # [doc = " Add item to batch, returns batch if full"] pub fn add (& mut self , item : T) -> Option < Vec < T > > { self . buffer . push (item) ; if self . buffer . len () >= self . batch_size { Some (self . flush ()) } else { None } } # [doc = " Flush remaining items"] pub fn flush (& mut self) -> Vec < T > { std :: mem :: take (& mut self . buffer) } # [doc = " Check if buffer is empty"] pub fn is_empty (& self) -> bool { self . buffer . is_empty () } # [doc = " Get current buffer size"] pub fn len (& self) -> usize { self . buffer . len () } } . self_ty`
- **Struct**: `LruCache` [fields: 3]
- **Impl**: `impl < K : Eq + Hash + Clone , V : Clone > LruCache < K , V > { # [doc = " Create cache with capacity"] pub fn new (capacity : usize) -> Self { Self { capacity , map : HashMap :: with_capacity (capacity) , ttl : None , } } # [doc = " Create cache with TTL"] pub fn with_ttl (capacity : usize , ttl : Duration) -> Self { Self { capacity , map : HashMap :: with_capacity (capacity) , ttl : Some (ttl) , } } # [doc = " Get value from cache"] pub fn get (& self , key : & K) -> Option < V > { self . map . get (key) . and_then (| (v , inserted) | { if let Some (ttl) = self . ttl { if inserted . elapsed () > ttl { return None ; } } Some (v . clone ()) }) } # [doc = " Insert value into cache"] pub fn insert (& mut self , key : K , value : V) { if self . map . len () >= self . capacity && ! self . map . contains_key (& key) { if let Some (oldest_key) = self . find_oldest () { self . map . remove (& oldest_key) ; } } self . map . insert (key , (value , Instant :: now ())) ; } # [doc = " Check if key exists"] pub fn contains (& self , key : & K) -> bool { self . get (key) . is_some () } # [doc = " Clear cache"] pub fn clear (& mut self) { self . map . clear () ; } # [doc = " Get cache size"] pub fn len (& self) -> usize { self . map . len () } # [doc = " Check if empty"] pub fn is_empty (& self) -> bool { self . map . is_empty () } fn find_oldest (& self) -> Option < K > { self . map . iter () . min_by_key (| (_ , (_ , instant)) | * instant) . map (| (k , _) | k . clone ()) } } . self_ty`
- **Struct**: `RingBuffer` [fields: 4]
- **Impl**: `impl < T : Clone > RingBuffer < T > { # [doc = " Create ring buffer with capacity"] pub fn new (capacity : usize) -> Self { Self { buffer : vec ! [None ; capacity] , head : 0 , tail : 0 , size : 0 , } } # [doc = " Push item (overwrites oldest if full)"] pub fn push (& mut self , item : T) { self . buffer [self . tail] = Some (item) ; self . tail = (self . tail + 1) % self . buffer . len () ; if self . size < self . buffer . len () { self . size += 1 ; } else { self . head = (self . head + 1) % self . buffer . len () ; } } # [doc = " Pop oldest item"] pub fn pop (& mut self) -> Option < T > { if self . size == 0 { return None ; } let item = self . buffer [self . head] . take () ; self . head = (self . head + 1) % self . buffer . len () ; self . size -= 1 ; item } # [doc = " Get current size"] pub fn len (& self) -> usize { self . size } # [doc = " Check if empty"] pub fn is_empty (& self) -> bool { self . size == 0 } # [doc = " Check if full"] pub fn is_full (& self) -> bool { self . size == self . buffer . len () } # [doc = " Get all items as vector"] pub fn to_vec (& self) -> Vec < T > { let mut result = Vec :: with_capacity (self . size) ; let mut idx = self . head ; for _ in 0 .. self . size { if let Some (ref item) = self . buffer [idx] { result . push (item . clone ()) ; } idx = (idx + 1) % self . buffer . len () ; } result } } . self_ty`
- **Struct**: `PerfStats` [fields: 4]
- **Impl**: `impl PerfStats { pub fn new () -> Self { Self { operation_count : 0 , total_duration_ns : 0 , min_duration_ns : u64 :: MAX , max_duration_ns : 0 , } } # [doc = " Record an operation duration"] pub fn record (& mut self , duration_ns : u64) { self . operation_count += 1 ; self . total_duration_ns += duration_ns ; self . min_duration_ns = self . min_duration_ns . min (duration_ns) ; self . max_duration_ns = self . max_duration_ns . max (duration_ns) ; } # [doc = " Get average duration in nanoseconds"] pub fn avg_ns (& self) -> u64 { if self . operation_count == 0 { 0 } else { self . total_duration_ns / self . operation_count } } # [doc = " Get average duration in microseconds"] pub fn avg_us (& self) -> f64 { self . avg_ns () as f64 / 1000.0 } # [doc = " Get average duration in milliseconds"] pub fn avg_ms (& self) -> f64 { self . avg_ns () as f64 / 1_000_000.0 } # [doc = " Get throughput (operations per second)"] pub fn throughput (& self) -> f64 { if self . total_duration_ns == 0 { 0.0 } else { self . operation_count as f64 / (self . total_duration_ns as f64 / 1_000_000_000.0) } } } . self_ty`
- **Struct**: `ScopedTimer` [fields: 2]
- **Impl**: `impl < 'a > ScopedTimer < 'a > { pub fn new (stats : & 'a mut PerfStats) -> Self { Self { stats , start : Instant :: now () , } } } . self_ty`
- **Impl**: `Drop` for `impl < 'a > Drop for ScopedTimer < 'a > { fn drop (& mut self) { let duration = self . start . elapsed () . as_nanos () as u64 ; self . stats . record (duration) ; } } . self_ty`
- **Function**: `process_chunks` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `estimate_memory_bytes` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `format_bytes` [complexity: 4] [cognitive: 3] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_batch_processor` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_lru_cache` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_ring_buffer` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_perf_stats` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_scoped_timer` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_estimate_memory` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_format_bytes` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_process_chunks` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]

### ./src/pmat.rs

**File Complexity**: 1 | **Functions**: 0

- **Struct**: `FileTdgScore` [fields: 3]
- **Struct**: `TdgAnalysis` [fields: 3]
- **Struct**: `PmatIntegration` [fields: 0]
- **Impl**: `impl PmatIntegration { # [doc = " Run pmat TDG analysis on a repository"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `repo_path` - Path to the repository to analyze"] # [doc = ""] # [doc = " # Returns"] # [doc = " * `Ok(TdgAnalysis)` with TDG scores"] # [doc = " * `Err` if pmat is not available or analysis fails"] # [doc = ""] # [doc = " # Examples"] # [doc = " ```no_run"] # [doc = " use organizational_intelligence_plugin::pmat::PmatIntegration;"] # [doc = " use std::path::PathBuf;"] # [doc = ""] # [doc = " let analysis = PmatIntegration::analyze_tdg(&PathBuf::from(\"/tmp/repo\")).unwrap();"] # [doc = " println!(\"Average TDG: {}\", analysis.average_score);"] # [doc = " ```"] pub fn analyze_tdg < P : AsRef < Path > > (repo_path : P) -> Result < TdgAnalysis > { let path = repo_path . as_ref () ; info ! ("Running pmat TDG analysis on {:?}" , path) ; if ! Self :: is_pmat_available () { warn ! ("pmat command not found - TDG analysis unavailable") ; return Err (anyhow ! ("pmat command not available in PATH")) ; } let output = Command :: new ("pmat") . args (["analyze" , "tdg" , "--path"]) . arg (path) . args (["--format" , "json"]) . output () . map_err (| e | anyhow ! ("Failed to execute pmat: {}" , e)) ? ; if ! output . status . success () { let stderr = String :: from_utf8_lossy (& output . stderr) ; return Err (anyhow ! ("pmat tdg failed: {}" , stderr)) ; } let stdout = String :: from_utf8_lossy (& output . stdout) ; debug ! ("pmat output: {}" , stdout) ; Self :: parse_tdg_output (& stdout) } # [doc = " Check if pmat command is available"] fn is_pmat_available () -> bool { Command :: new ("pmat") . arg ("--version") . output () . map (| output | output . status . success ()) . unwrap_or (false) } # [doc = " Parse pmat TDG JSON output"] fn parse_tdg_output (json_output : & str) -> Result < TdgAnalysis > { # [derive (Deserialize)] struct PmatFile { file_path : String , total : f32 , # [allow (dead_code)] # [serde (default)] grade : String , } # [derive (Deserialize)] struct PmatOutput { files : Vec < PmatFile > , } let parsed : PmatOutput = serde_json :: from_str (json_output) . map_err (| e | anyhow ! ("Failed to parse pmat JSON: {}" , e)) ? ; let mut file_scores = HashMap :: new () ; let mut total_score = 0.0_f32 ; let mut max_score = 0.0_f32 ; for file in & parsed . files { file_scores . insert (file . file_path . clone () , file . total) ; total_score += file . total ; max_score = max_score . max (file . total) ; } let average_score = if parsed . files . is_empty () { 0.0 } else { total_score / parsed . files . len () as f32 } ; Ok (TdgAnalysis { file_scores , average_score , max_score , }) } # [doc = " Get TDG score for a specific file"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `analysis` - TDG analysis result"] # [doc = " * `file_path` - Path to look up"] # [doc = ""] # [doc = " # Returns"] # [doc = " * `Some(score)` if file was analyzed"] # [doc = " * `None` if file not found"] pub fn get_file_score (analysis : & TdgAnalysis , file_path : & str) -> Option < f32 > { analysis . file_scores . get (file_path) . copied () } } . self_ty`
- **Struct**: `PmatFile` [fields: 3]
- **Struct**: `PmatOutput` [fields: 1]
- **Function**: `tests::test_parse_tdg_output` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_parse_empty_tdg_output` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_get_file_score` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_analyze_tdg_integration` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]

### ./src/pr_reviewer.rs

**File Complexity**: 6 | **Functions**: 4

- **Struct**: `PrWarning` [fields: 6]
- **Struct**: `PrReview` [fields: 4]
- **Impl**: `impl PrReview { # [doc = " Generate markdown report suitable for GitHub comments"] pub fn to_markdown (& self) -> String { let mut output = String :: new () ; output . push_str ("# PR Review: Organizational Intelligence\n\n") ; if self . warnings . is_empty () { output . push_str (" **No warnings** - No files match historical defect patterns.\n\n") ; } else { output . push_str (& format ! ("##  {} Warning{} Based on Historical Patterns\n\n" , self . warnings . len () , if self . warnings . len () == 1 { "" } else { "s" })) ; for warning in & self . warnings { output . push_str (& format ! ("### {}\n" , warning . file)) ; output . push_str (& format ! ("**Category**: {} ({} occurrences, TDG: {:.1})\n\n" , warning . category , warning . frequency , warning . avg_tdg_score)) ; output . push_str (& format ! ("{}\n\n" , warning . message)) ; if ! warning . prevention_tips . is_empty () { output . push_str ("**Prevention Strategies**:\n") ; for tip in & warning . prevention_tips { output . push_str (& format ! ("-  {}\n" , tip)) ; } output . push ('\n') ; } output . push_str ("---\n\n") ; } } output . push_str (& format ! ("**Analysis Date**: {} (baseline is from {} repositories)\n" , self . baseline_date , self . repositories_analyzed)) ; output . push_str (& format ! ("**Files Analyzed**: {}\n" , self . files_analyzed . len ())) ; output } # [doc = " Generate JSON report for programmatic consumption"] pub fn to_json (& self) -> Result < String > { serde_json :: to_string_pretty (self) . context ("Failed to serialize PR review to JSON") } } . self_ty`
- **Struct**: `PrReviewer` [fields: 1]
- **Impl**: `impl PrReviewer { # [doc = " Load baseline summary from weekly analysis"] pub fn load_baseline < P : AsRef < Path > > (path : P) -> Result < Self > { let baseline = Summary :: from_file (path . as_ref ()) . context ("Failed to load baseline summary") ? ; Ok (Self { baseline }) } # [doc = " Review PR files against organizational defect patterns"] pub fn review_pr (& self , files_changed : & [String]) -> PrReview { let mut warnings = Vec :: new () ; for file in files_changed { let file_warnings = self . check_file_patterns (file) ; warnings . extend (file_warnings) ; } PrReview { warnings , baseline_date : self . baseline . metadata . analysis_date . clone () , repositories_analyzed : self . baseline . metadata . repositories_analyzed , files_analyzed : files_changed . to_vec () , } } # [doc = " Check a single file against all defect patterns"] fn check_file_patterns (& self , file : & str) -> Vec < PrWarning > { let mut warnings = Vec :: new () ; if is_config_file (file) { if let Some (pattern) = self . baseline . find_category ("ConfigurationErrors") { if should_warn (& pattern) { warnings . push (PrWarning { file : file . to_string () , category : pattern . category . clone () , message : format ! ("This org has {} config errors (avg TDG: {:.1}). Ensure validation!" , pattern . frequency , pattern . avg_tdg_score) , prevention_tips : pattern . prevention_strategies . clone () , frequency : pattern . frequency , avg_tdg_score : pattern . avg_tdg_score , }) ; } } } if is_integration_file (file) { if let Some (pattern) = self . baseline . find_category ("IntegrationFailures") { if should_warn (& pattern) { warnings . push (PrWarning { file : file . to_string () , category : pattern . category . clone () , message : format ! ("Integration issues detected {} times (avg TDG: {:.1}). Check timeouts and retries!" , pattern . frequency , pattern . avg_tdg_score) , prevention_tips : pattern . prevention_strategies . clone () , frequency : pattern . frequency , avg_tdg_score : pattern . avg_tdg_score , }) ; } } } if is_code_file (file) { if let Some (pattern) = self . baseline . find_category ("LogicErrors") { if should_warn (& pattern) { warnings . push (PrWarning { file : file . to_string () , category : pattern . category . clone () , message : format ! ("Logic errors occurred {} times (avg TDG: {:.1}). Test edge cases!" , pattern . frequency , pattern . avg_tdg_score) , prevention_tips : pattern . prevention_strategies . clone () , frequency : pattern . frequency , avg_tdg_score : pattern . avg_tdg_score , }) ; } } } warnings } } . self_ty`
- **Function**: `is_config_file` [complexity: 7] [cognitive: 6] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `is_integration_file` [complexity: 5] [cognitive: 4] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `is_code_file` [complexity: 10] [cognitive: 9] [big-o: O(n log n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `should_warn` [complexity: 3] [cognitive: 2] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::create_test_summary` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_config_file_triggers_warning` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_integration_file_triggers_warning` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_low_frequency_no_warning` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_no_warnings_for_unmatched_files` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_multiple_files_multiple_warnings` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_markdown_output_with_warnings` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_markdown_output_no_warnings` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_json_output` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `tests::test_file_type_detection` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]

### ./src/query.rs

**File Complexity**: 1 | **Functions**: 0

- **Enum**: `QueryType` [variants: 4]
- **Struct**: `Query` [fields: 2]
- **Struct**: `QueryParser` [fields: 0]
- **Impl**: `impl QueryParser { pub fn new () -> Self { Self } # [doc = " Parse natural language query into structured query"] pub fn parse (& self , input : & str) -> Result < Query > { let lower = input . to_lowercase () ; let query_type = if lower . contains ("most common") || lower . contains ("top defect") { QueryType :: MostCommonDefect } else if lower . contains ("count") && (lower . contains ("category") || lower . contains ("defect")) { QueryType :: CountByCategory } else if lower . contains ("show") && lower . contains ("all") { QueryType :: ListAll } else { QueryType :: Unknown (input . to_string ()) } ; Ok (Query { query_type , original : input . to_string () , }) } } . self_ty`
- **Impl**: `Default` for `impl Default for QueryParser { fn default () -> Self { Self :: new () } } . self_ty`
- **Function**: `tests::test_parser_creation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_most_common_defect` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_count_by_category` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_list_all` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_unknown_query` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]

### ./src/report.rs

**File Complexity**: 1 | **Functions**: 0

- **Struct**: `AnalysisMetadata` [fields: 5]
- **Struct**: `QualitySignals` [fields: 7]
- **Impl**: `Default` for `impl Default for QualitySignals { fn default () -> Self { Self { avg_tdg_score : None , max_tdg_score : None , avg_complexity : None , avg_test_coverage : None , satd_instances : 0 , avg_lines_changed : 0.0 , avg_files_per_commit : 0.0 , } } } . self_ty`
- **Struct**: `DefectInstance` [fields: 7]
- **Struct**: `DefectPattern` [fields: 5]
- **Struct**: `AnalysisReport` [fields: 3]
- **Struct**: `ReportGenerator` [fields: 0]
- **Impl**: `impl ReportGenerator { # [doc = " Create a new report generator"] # [doc = ""] # [doc = " # Examples"] # [doc = " ```"] # [doc = " use organizational_intelligence_plugin::report::ReportGenerator;"] # [doc = ""] # [doc = " let generator = ReportGenerator::new();"] # [doc = " ```"] pub fn new () -> Self { Self } # [doc = " Convert report to YAML string"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `report` - The analysis report to serialize"] # [doc = ""] # [doc = " # Errors"] # [doc = " Returns error if serialization fails"] # [doc = ""] # [doc = " # Examples"] # [doc = " ```"] # [doc = " use organizational_intelligence_plugin::report::{"] # [doc = "     ReportGenerator, AnalysisReport, AnalysisMetadata"] # [doc = " };"] # [doc = ""] # [doc = " let generator = ReportGenerator::new();"] # [doc = " let metadata = AnalysisMetadata {"] # [doc = "     organization: \"test-org\".to_string(),"] # [doc = "     analysis_date: \"2025-11-15T00:00:00Z\".to_string(),"] # [doc = "     repositories_analyzed: 10,"] # [doc = "     commits_analyzed: 100,"] # [doc = "     analyzer_version: \"0.1.0\".to_string(),"] # [doc = " };"] # [doc = ""] # [doc = " let report = AnalysisReport {"] # [doc = "     version: \"1.0\".to_string(),"] # [doc = "     metadata,"] # [doc = "     defect_patterns: vec![],"] # [doc = " };"] # [doc = ""] # [doc = " let yaml = generator.to_yaml(&report).expect(\"Should serialize\");"] # [doc = " assert!(yaml.contains(\"version\"));"] # [doc = " ```"] pub fn to_yaml (& self , report : & AnalysisReport) -> Result < String > { debug ! ("Serializing report to YAML") ; let yaml = serde_yaml :: to_string (report) ? ; Ok (yaml) } # [doc = " Write report to file"] # [doc = ""] # [doc = " # Arguments"] # [doc = " * `report` - The analysis report to write"] # [doc = " * `path` - Path to output file"] # [doc = ""] # [doc = " # Errors"] # [doc = " Returns error if:"] # [doc = " - Serialization fails"] # [doc = " - File write fails"] # [doc = " - Path is invalid"] # [doc = ""] # [doc = " # Examples"] # [doc = " ```no_run"] # [doc = " use organizational_intelligence_plugin::report::{"] # [doc = "     ReportGenerator, AnalysisReport, AnalysisMetadata"] # [doc = " };"] # [doc = " use std::path::PathBuf;"] # [doc = ""] # [doc = " # async fn example() -> Result<(), anyhow::Error> {"] # [doc = " let generator = ReportGenerator::new();"] # [doc = " let metadata = AnalysisMetadata {"] # [doc = "     organization: \"test-org\".to_string(),"] # [doc = "     analysis_date: \"2025-11-15T00:00:00Z\".to_string(),"] # [doc = "     repositories_analyzed: 10,"] # [doc = "     commits_analyzed: 100,"] # [doc = "     analyzer_version: \"0.1.0\".to_string(),"] # [doc = " };"] # [doc = ""] # [doc = " let report = AnalysisReport {"] # [doc = "     version: \"1.0\".to_string(),"] # [doc = "     metadata,"] # [doc = "     defect_patterns: vec![],"] # [doc = " };"] # [doc = ""] # [doc = " generator.write_to_file(&report, &PathBuf::from(\"report.yaml\")).await?;"] # [doc = " # Ok(())"] # [doc = " # }"] # [doc = " ```"] pub async fn write_to_file (& self , report : & AnalysisReport , path : & Path) -> Result < () > { info ! ("Writing report to file: {}" , path . display ()) ; let yaml = self . to_yaml (report) ? ; fs :: write (path , yaml) . await ? ; info ! ("Successfully wrote report to {}" , path . display ()) ; Ok (()) } } . self_ty`
- **Impl**: `Default` for `impl Default for ReportGenerator { fn default () -> Self { Self :: new () } } . self_ty`
- **Function**: `tests::test_report_generator_creation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_yaml_serialization` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_yaml_with_defect_patterns` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]

### ./src/sliding_window.rs

**File Complexity**: 5 | **Functions**: 1

- **Struct**: `TimeWindow` [fields: 2]
- **Impl**: `impl TimeWindow { # [doc = " Create time window"] pub fn new (start_time : f64 , end_time : f64) -> Self { Self { start_time , end_time , } } # [doc = " Create 6-month window starting at given time"] pub fn six_months_from (start_time : f64) -> Self { Self { start_time , end_time : start_time + SIX_MONTHS_SECONDS , } } # [doc = " Check if window contains timestamp"] pub fn contains (& self , timestamp : f64) -> bool { timestamp >= self . start_time && timestamp < self . end_time } # [doc = " Get window duration in seconds"] pub fn duration (& self) -> f64 { self . end_time - self . start_time } } . self_ty`
- **Struct**: `WindowedCorrelationMatrix` [fields: 3]
- **Struct**: `SlidingWindowAnalyzer` [fields: 2]
- **Impl**: `impl SlidingWindowAnalyzer { # [doc = " Create analyzer with 6-month windows, 3-month stride (50% overlap)"] pub fn new_six_month () -> Self { Self { window_size : SIX_MONTHS_SECONDS , stride : SIX_MONTHS_SECONDS / 2.0 , } } # [doc = " Create analyzer with custom window size and stride"] pub fn new (window_size : f64 , stride : f64) -> Self { Self { window_size , stride , } } # [doc = " Generate time windows for given data range"] pub fn generate_windows (& self , start_time : f64 , end_time : f64) -> Vec < TimeWindow > { let mut windows = Vec :: new () ; let mut current_start = start_time ; while current_start + self . window_size <= end_time { windows . push (TimeWindow :: new (current_start , current_start + self . window_size ,)) ; current_start += self . stride ; } windows } # [doc = " Compute correlation matrix for features in a time window"] # [doc = ""] # [doc = " Returns correlation matrix between all feature dimensions"] pub fn compute_window_correlation (& self , store : & FeatureStore , window : & TimeWindow ,) -> Result < WindowedCorrelationMatrix > { let features = store . query_by_time_range (window . start_time , window . end_time) ? ; if features . is_empty () { anyhow :: bail ! ("No features in window [{}, {})" , window . start_time , window . end_time) ; } let vectors : Vec < Vec < f32 > > = features . iter () . map (| f | f . to_vector ()) . collect () ; let n_samples = vectors . len () ; let n_dims = CommitFeatures :: DIMENSION ; let mut dim_arrays : Vec < Vec < f32 > > = vec ! [Vec :: new () ; n_dims] ; for v in & vectors { for (dim_idx , & value) in v . iter () . enumerate () { dim_arrays [dim_idx] . push (value) ; } } let mut matrix = vec ! [vec ! [0.0 ; n_dims] ; n_dims] ; for i in 0 .. n_dims { for j in 0 .. n_dims { if i == j { matrix [i] [j] = 1.0 ; } else { let vec_i = Vector :: from_slice (& dim_arrays [i]) ; let vec_j = Vector :: from_slice (& dim_arrays [j]) ; matrix [i] [j] = pearson_correlation (& vec_i , & vec_j) ? ; } } } Ok (WindowedCorrelationMatrix { window : window . clone () , matrix , feature_count : n_samples , }) } # [doc = " Compute correlation matrices for all windows"] pub fn compute_all_windows (& self , store : & FeatureStore ,) -> Result < Vec < WindowedCorrelationMatrix > > { let all_features = store . all_features () ; if all_features . is_empty () { anyhow :: bail ! ("No features in store") ; } let start_time = all_features . iter () . map (| f | f . timestamp) . fold (f64 :: INFINITY , f64 :: min) ; let end_time = all_features . iter () . map (| f | f . timestamp) . fold (f64 :: NEG_INFINITY , f64 :: max) ; let windows = self . generate_windows (start_time , end_time) ; let mut results = Vec :: new () ; for window in windows { match self . compute_window_correlation (store , & window) { Ok (wcm) => results . push (wcm) , Err (_) => continue , } } Ok (results) } } . self_ty`
- **Struct**: `ConceptDrift` [fields: 4]
- **Function**: `detect_drift` [complexity: 5] [cognitive: 9] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_time_window_creation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_six_month_window` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_generate_windows` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `tests::test_window_correlation_computation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]

### ./src/storage.rs

**File Complexity**: 1 | **Functions**: 0

- **Struct**: `FeatureStore` [fields: 1]
- **Impl**: `impl FeatureStore { # [doc = " Create new feature store"] pub fn new () -> Result < Self > { Ok (Self { features : Vec :: new () , }) } # [doc = " Insert single feature"] pub fn insert (& mut self , feature : CommitFeatures) -> Result < () > { self . features . push (feature) ; Ok (()) } # [doc = " Bulk insert features (optimized for GPU batch processing)"] pub fn bulk_insert (& mut self , features : Vec < CommitFeatures >) -> Result < () > { self . features . extend (features) ; Ok (()) } # [doc = " Query features by defect category"] pub fn query_by_category (& self , category : u8) -> Result < Vec < & CommitFeatures > > { Ok (self . features . iter () . filter (| f | f . defect_category == category) . collect ()) } # [doc = " Query features by time range (for sliding window correlation)"] # [doc = ""] # [doc = " Returns features where start_time <= timestamp < end_time"] # [doc = " Time is in Unix epoch seconds (f64)"] pub fn query_by_time_range (& self , start_time : f64 , end_time : f64 ,) -> Result < Vec < & CommitFeatures > > { Ok (self . features . iter () . filter (| f | f . timestamp >= start_time && f . timestamp < end_time) . collect ()) } # [doc = " Get all features (for compatibility)"] pub fn all_features (& self) -> & [CommitFeatures] { & self . features } # [doc = " Get all features as vectors (for GPU transfer)"] pub fn to_vectors (& self) -> Vec < Vec < f32 > > { self . features . iter () . map (| f | f . to_vector ()) . collect () } # [doc = " Get feature count"] pub fn len (& self) -> usize { self . features . len () } # [doc = " Check if store is empty"] pub fn is_empty (& self) -> bool { self . features . is_empty () } # [doc = " Save to file (JSON in Phase 1, Parquet in Phase 2)"] pub async fn save (& self , path : & Path) -> Result < () > { let json = serde_json :: to_string (& self . features) ? ; std :: fs :: write (path , json) ? ; Ok (()) } # [doc = " Load from file (JSON in Phase 1, Parquet in Phase 2)"] pub async fn load (path : & Path) -> Result < Self > { if ! path . exists () { return Self :: new () ; } let json = std :: fs :: read_to_string (path) ? ; let features : Vec < CommitFeatures > = serde_json :: from_str (& json) ? ; Ok (Self { features }) } } . self_ty`
- **Impl**: `Default` for `impl Default for FeatureStore { fn default () -> Self { Self :: new () . unwrap () } } . self_ty`
- **Function**: `tests::make_test_feature` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_store_creation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_insert_single` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_bulk_insert` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_query_by_category` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_to_vectors` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_query_by_time_range` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_save_load` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]

### ./src/summarizer.rs

**File Complexity**: 1 | **Functions**: 0

- **Struct**: `SummaryConfig` [fields: 4]
- **Impl**: `Default` for `impl Default for SummaryConfig { fn default () -> Self { Self { strip_pii : true , top_n_categories : 10 , min_frequency : 5 , include_examples : false , } } } . self_ty`
- **Struct**: `QualityThresholds` [fields: 4]
- **Impl**: `Default` for `impl Default for QualityThresholds { fn default () -> Self { Self { tdg_minimum : 85.0 , test_coverage_minimum : 0.85 , max_function_length : 50 , max_cyclomatic_complexity : 10 , } } } . self_ty`
- **Struct**: `SummaryMetadata` [fields: 3]
- **Struct**: `Summary` [fields: 3]
- **Impl**: `impl Summary { # [doc = " Find a defect category by name"] pub fn find_category (& self , category_name : & str) -> Option < DefectPatternSummary > { self . organizational_insights . top_defect_categories . iter () . find (| p | p . category . to_string () == category_name) . map (| p | DefectPatternSummary { category : p . category . to_string () , frequency : p . frequency , confidence : p . confidence , avg_tdg_score : p . quality_signals . avg_tdg_score . unwrap_or (0.0) , common_patterns : Vec :: new () , prevention_strategies : Vec :: new () , }) } # [doc = " Load summary from file"] pub fn from_file < P : AsRef < Path > > (path : P) -> Result < Self > { let content = std :: fs :: read_to_string (path) ? ; let summary : Summary = serde_yaml :: from_str (& content) ? ; Ok (summary) } } . self_ty`
- **Struct**: `OrganizationalInsights` [fields: 1]
- **Struct**: `DefectPatternSummary` [fields: 6]
- **Struct**: `ReportSummarizer` [fields: 0]
- **Impl**: `impl ReportSummarizer { # [doc = " Summarize a full organizational report according to config"] pub fn summarize < P : AsRef < Path > > (input : P , config : SummaryConfig) -> Result < Summary > { let content = std :: fs :: read_to_string (input) ? ; let report : AnalysisReport = serde_yaml :: from_str (& content) ? ; let mut patterns : Vec < DefectPattern > = report . defect_patterns . into_iter () . filter (| p | p . frequency >= config . min_frequency) . collect () ; patterns . sort_by (| a , b | b . frequency . cmp (& a . frequency)) ; patterns . truncate (config . top_n_categories) ; if config . strip_pii { Self :: strip_pii_from_patterns (& mut patterns) ; } if ! config . include_examples { for pattern in & mut patterns { pattern . examples . clear () ; } } Ok (Summary { organizational_insights : OrganizationalInsights { top_defect_categories : patterns , } , code_quality_thresholds : QualityThresholds :: default () , metadata : SummaryMetadata { analysis_date : report . metadata . analysis_date , repositories_analyzed : report . metadata . repositories_analyzed , commits_analyzed : report . metadata . commits_analyzed , } , }) } # [doc = " Strip PII from defect patterns (author, commit hash, email)"] fn strip_pii_from_patterns (patterns : & mut [DefectPattern]) { for pattern in patterns { for example in & mut pattern . examples { example . commit_hash = "REDACTED" . to_string () ; example . author = "REDACTED" . to_string () ; } } } # [doc = " Save summary to YAML file"] pub fn save_to_file < P : AsRef < Path > > (summary : & Summary , output : P) -> Result < () > { let yaml = serde_yaml :: to_string (summary) ? ; std :: fs :: write (output , yaml) ? ; Ok (()) } } . self_ty`
- **Function**: `tests::create_test_report` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_pii_stripping_removes_sensitive_data` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_frequency_filtering` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_top_n_selection` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_examples_removed_by_default` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]
- **Function**: `tests::test_roundtrip_save_and_load` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(3)] [tdg: 2.5]

### ./tests/cli_integration.rs

**File Complexity**: 1 | **Functions**: 16

- **Function**: `oip_gpu_bin` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `run_oip_gpu` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `output_contains` [complexity: 2] [cognitive: 1] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_cli_help` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_cli_version` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_analyze_help` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_correlate_help` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_predict_help` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_query_help` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_cluster_help` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_benchmark_help` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_invalid_command` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_analyze_missing_target` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_verbose_flag` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_backend_flag_simd` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_backend_flag_cpu` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `workflow::test_output_directory_creation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `modules::make_feature` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `modules::test_full_pipeline_feature_to_prediction` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `modules::test_full_pipeline_clustering` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `modules::test_full_pipeline_correlation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `modules::test_full_pipeline_imbalance_handling` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `modules::test_full_pipeline_sliding_window` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `modules::test_metrics_evaluation` [complexity: 3] [cognitive: 2] [big-o: O(n)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]

### ./tests/cli_tests.rs

**File Complexity**: 1 | **Functions**: 8

- **Function**: `test_cli_requires_subcommand` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `test_cli_analyze_command_requires_org` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `test_cli_analyze_command_with_org` [complexity: 2] [cognitive: 1] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `test_cli_analyze_command_with_output` [complexity: 2] [cognitive: 1] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `test_cli_global_verbose_flag` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `test_cli_summarize_command_requires_input_and_output` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `test_cli_summarize_command_with_required_args` [complexity: 2] [cognitive: 1] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]
- **Function**: `test_cli_summarize_command_with_all_options` [complexity: 2] [cognitive: 1] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(2)] [tdg: 2.5]

### ./tests/unit/classifier_test.rs

**File Complexity**: 1 | **Functions**: 10

- **Function**: `test_classifier_can_be_created` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_defect_categories_exist` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_classify_memory_safety_from_commit_message` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_classify_concurrency_from_commit_message` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_classify_security_from_commit_message` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_classification_includes_explanation` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_classification_confidence_range` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_unclassifiable_message_returns_none` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_case_insensitive_matching` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_multiple_pattern_matches_uses_highest_confidence` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]

### ./tests/unit/cli_test.rs

**File Complexity**: 1 | **Functions**: 3

- **Function**: `test_cli_parses_version_flag` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_cli_requires_subcommand` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_cli_help_flag` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]

### ./tests/unit/github_miner_test.rs

**File Complexity**: 1 | **Functions**: 4

- **Function**: `test_github_miner_can_be_created` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_github_miner_with_token` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_fetch_organization_repos_validates_org_name` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_fetch_organization_repos_returns_repo_list` [complexity: 2] [cognitive: 1] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]

### ./tests/unit/report_test.rs

**File Complexity**: 1 | **Functions**: 6

- **Function**: `test_report_generator_can_be_created` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_analysis_report_structure` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_report_generator_to_yaml` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_report_generator_write_to_file` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_defect_pattern_structure` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]
- **Function**: `test_empty_report_is_valid` [complexity: 1] [cognitive: 0] [big-o: O(1)] [provability: 43%] [satd: 0] [churn: low(1)] [tdg: 2.5]

